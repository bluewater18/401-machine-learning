{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 401 Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# imports\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms\n",
    "import torch.optim\n",
    "import torch.utils.data.sampler\n",
    "import torch.nn.functional\n",
    "\n",
    "\n",
    "# settings\n",
    "\n",
    "random.seed(55)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Gradient-based Learning with Tensors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining R<sup>5</sup> to R<sup>4</sup> Function\n",
    "The definition of the function with noise added is done in create_output_tensor_noisy. This takes a tensor input of\n",
    "with many different 5 value tuples and returns the output for all the tuples passed in.\n",
    "The R<sup>5</sup> to R<sup>4</sup> is defined by multiplying the input with a predefined weight matrix to apply\n",
    "a linear change.\n",
    " "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def create_output_tensor_noisy(input, weights):\n",
    "    tensor_a = input#torch.from_numpy(input)\n",
    "    tensor_b = torch.from_numpy(weights)\n",
    "    output = tensor_a.t()@tensor_b.t()\n",
    "    noise = np.random.normal(loc=0, scale=0.01, size=np.shape(input.size))\n",
    "    output = output + torch.from_numpy(noise)\n",
    "    return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training\n",
    "We then train the network using tenor operations, our prediction / model is defined as transpose of input (x) matrix multiplied by\n",
    "the transpose of our current weight matrix then the addition of our biases.\n",
    "We calculate our loss using Mean Square Error (mse).\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "epoch 100 loss = 87.98234558105469\nepoch 200 loss = 36.75639724731445\nepoch 300 loss = 16.071008682250977\nepoch 400 loss = 7.328232288360596\nepoch 500 loss = 3.4663479328155518\nepoch 600 loss = 1.6904804706573486\n",
      "epoch 700 loss = 0.8449733257293701\nepoch 800 loss = 0.43064481019973755\nepoch 900 loss = 0.2228463590145111\nepoch 1000 loss = 0.1166977658867836\n",
      "epoch 1100 loss = 0.06168539449572563\nepoch 1200 loss = 0.0328509584069252\nepoch 1300 loss = 0.01759958826005459\nepoch 1400 loss = 0.009475289843976498\nepoch 1500 loss = 0.00512198219075799\nepoch 1600 loss = 0.0027779724914580584\n",
      "epoch 1700 loss = 0.0015110061503946781\nepoch 1800 loss = 0.000823843467514962\nepoch 1900 loss = 0.00045010432950221\nepoch 2000 loss = 0.00024632096756249666\nExpected:\n[[ 5.861115   10.268809   -1.5568438  10.771001  ]\n [ 1.0745783   8.165716   -8.592041   -5.300141  ]\n [ 3.180463   25.95541    -9.765273   -5.9001365 ]\n [ 9.077727   20.789263   10.798818    8.221809  ]\n [ 3.4222834  -0.77491516 24.398956   11.715445  ]]\nPrediction:\n[[ 5.8465366  10.264385   -1.566852   10.755089  ]\n [ 1.0652262   8.183075   -8.588157   -5.3064456 ]\n [ 3.1685734  25.956226   -9.766713   -5.908792  ]\n [ 9.05842    20.759468   10.771976    8.198493  ]\n [ 3.4021     -0.81569177 24.371168   11.693127  ]]\n**********\nFinal MSE:\n0.000355653406586498\n**********\nKnown Linear Weights:\n[[ 3.7  2.3  1.   0.   5. ]\n [ 4.7  8.1  2.5  0.  25. ]\n [ 2.7  1.   9.   3.9  0. ]\n [ 5.2  6.   2.   0.   0. ]]\n****\nFound Linear Weights:\ntensor([[ 3.6984e+00,  2.2977e+00,  9.9837e-01, -4.2869e-03,  4.9973e+00],\n        [ 4.6962e+00,  8.0881e+00,  2.4895e+00, -2.2574e-02,  2.4985e+01],\n        [ 2.6947e+00,  9.9549e-01,  8.9956e+00,  3.8860e+00, -6.5074e-03],\n        [ 5.1962e+00,  5.9969e+00,  1.9985e+00, -7.0744e-03, -3.2393e-03]],\n       requires_grad=True)\n**********\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def mse(in1, in2):\n",
    "    diff = in1 - in2\n",
    "    return torch.sum(diff*diff) / diff.numel()\n",
    "\n",
    "def model(x,w,b):\n",
    "    return x.t() @ w.t() +b\n",
    "\n",
    "def train(input, target, w, b, i, learning_rate=1e-2):\n",
    "    prediction = model(input, w,b)\n",
    "    loss = mse(prediction, target)\n",
    "    if i % 100 == 0:\n",
    "        print(\"epoch \" + str(i) + \" loss = \" + str(loss.item()))\n",
    "    loss.backward(retain_graph=True)\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * learning_rate\n",
    "        b -= b.grad * learning_rate\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "def test(input, target, w, b):\n",
    "    prediction = model(input, w, b)\n",
    "    print('Expected:')\n",
    "    print(target.data.numpy())\n",
    "    print('Prediction:')\n",
    "    print(prediction.data.numpy())\n",
    "    return prediction\n",
    "\n",
    "\n",
    "\n",
    "eqtn = np.array([[3.7, 2.3, 1, 0, 5],\n",
    "      [4.7, 8.1, 2.5,  0, 25],\n",
    "      [2.7, 1, 9, 3.9, 0],\n",
    "      [5.2, 6, 2, 0, 0]], dtype=np.float32)\n",
    "tensor_in = torch.randn(5,100, requires_grad=True)\n",
    "tensor_target = create_output_tensor_noisy(tensor_in, eqtn)     \n",
    "weights = torch.randn(4,5, requires_grad=True)\n",
    "biases = torch.randn(4, requires_grad=True)\n",
    "q1_epochs = 2000\n",
    "for i in range(1,q1_epochs+1):\n",
    "    train(input= tensor_in, target= tensor_target, w = weights, b = biases, i = i)\n",
    "\n",
    "\n",
    "tensor_test_in = torch.randn(5,5, requires_grad=True)\n",
    "tensor_test_target = create_output_tensor_noisy(tensor_test_in, eqtn)  \n",
    "q1_test_pred = test(tensor_test_in, tensor_test_target, weights, biases)\n",
    "print('**********')\n",
    "print('Final MSE:')\n",
    "print(mse(q1_test_pred, tensor_test_target).item())\n",
    "\n",
    "print('**********')\n",
    "print('Known Linear Weights:')\n",
    "print(eqtn)\n",
    "print('****')\n",
    "print('Found Linear Weights:')\n",
    "print(weights)\n",
    "print('**********')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Transfer Learning\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MNIST\n",
    "\n",
    "Transfer learning starts with the creation of a CNN for MNIST dataset, where we are able to train it to 99% correctness\n",
    "in one epoch of the 60,000 point dataset. This is a large increase from the 6% accuracy it has before training.\n",
    "\n",
    "Our CNN is defined in the class MNISTCNN which extends pytorchs Module class, provinding the underlying logic for many\n",
    "CNN opertaions, allowing us to better define parameters such as inputs, layers and the foward function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "seed = 55\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "class MNISTCNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTCNN,self).__init__()\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1,32, kernel_size=5, stride=1, padding=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32,64, kernel_size=5, stride=1, padding=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.drop_out = torch.nn.Dropout()\n",
    "        self.fc1 = torch.nn.Linear(7 * 7 * 64, 1000)\n",
    "        self.fc2 = torch.nn.Linear(1000, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.drop_out(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " Next we train and test the CNN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Training model with:\nbatch size = 64\nlearning rate = 0.001\nover 1 epochs\n",
      "Before Training\n",
      "\nTest set: Average loss: 0.0364, Accuracy: 623/10000 (6%)\n\nTraining model with:\nbatch size = 64\nlearning rate = 0.001\nover 1 epochs\nTrain Epoch: 1 [0/60000 (0%)]\tLoss: 2.320465\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.253857\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.072163\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.061900\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.065473\n",
      "\nTest set: Average loss: 0.0006, Accuracy: 9878/10000 (99%)\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def train_net(mnist_model, device, train_loader, optimizer, loss_func, epoch):\n",
    "    mnist_model.train()\n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = mnist_model(data)\n",
    "        loss = loss_func(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 200 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(data), len(train_loader.dataset),\n",
    "                100. * i / len(train_loader), loss.item()))\n",
    "    return loss.item()\n",
    "\n",
    "def test_net(mnist_model, device, test_loader, loss_func):\n",
    "    mnist_model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = mnist_model(data)\n",
    "            _, pred = torch.max(output.data,1)\n",
    "            test_loss += loss_func(output, target).item() # sum up batch loss\n",
    "            correct += pred.eq(target).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return correct / len(test_loader.dataset)\n",
    "\n",
    "batch_size = 64\n",
    "train_batch_size = 64\n",
    "test_batch_size = 100\n",
    "mnist_epochs = 1\n",
    "device = torch.device(\"cpu\")\n",
    "learning_rate = 1e-3\n",
    "\n",
    "print(\"Training model with:\")\n",
    "print(\"batch size = \" + str(train_batch_size))\n",
    "print(\"learning rate = \" + str(learning_rate))\n",
    "print(\"over \" + str(mnist_epochs) + \" epochs\")\n",
    "\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "            torchvision.datasets.MNIST('./data', train=True, download=True,\n",
    "               transform=torchvision.transforms.transforms.Compose([\n",
    "                   torchvision.transforms.transforms.ToTensor(),\n",
    "                   torchvision.transforms.transforms.Normalize((0.1307,), (0.3081,))\n",
    "               ])),\n",
    "            batch_size=train_batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "            torchvision.datasets.MNIST('./data', train=False, transform=torchvision.transforms.transforms.Compose([\n",
    "                torchvision.transforms.transforms.ToTensor(),\n",
    "                torchvision.transforms.transforms.Normalize((0.1307,), (0.3081,))\n",
    "            ])),\n",
    "            batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "mnist_model = MNISTCNN().to(device)\n",
    "optimizer = torch.optim.Adam(mnist_model.parameters(), lr=learning_rate)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "print(\"Before Training\")\n",
    "test_net(mnist_model,device,test_loader,loss_func)\n",
    "print(\"Training model with:\")\n",
    "print(\"batch size = \" + str(train_batch_size))\n",
    "print(\"learning rate = \" + str(learning_rate))\n",
    "print(\"over \" + str(mnist_epochs) + \" epochs\")\n",
    "for epoch in range(1, mnist_epochs + 1):\n",
    "    train_net(mnist_model, device, train_loader, optimizer, loss_func, epoch)\n",
    "    test_net(mnist_model, device, test_loader, loss_func)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CIFAR10\n",
    "To complete transfer learning we have to create a compatible CNN for our second data set, CIFAR10. This requires some\n",
    "form of transformation of the data as the data sets include images of different sizes and channels.\n",
    "This transformation can be seen in the transform variable below, where we use the Compose function to add several \n",
    "transforms including resizing the image and making it greyscale  so it has the same input parameters as the MNIST\n",
    "dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "seed = 55\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cpu\")\n",
    "cifar_epoch = 10\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(size=28),\n",
    "    torchvision.transforms.Grayscale(num_output_channels=1),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.1307,), (0.3081,))])#((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "#Training\n",
    "n_training_samples = 40000\n",
    "train_sampler = torch.utils.data.sampler.SubsetRandomSampler(np.arange(n_training_samples, dtype=np.int64))\n",
    "\n",
    "#Validation\n",
    "n_val_samples = 5000\n",
    "val_sampler = torch.utils.data.sampler.SubsetRandomSampler(np.arange(n_training_samples, n_training_samples + n_val_samples, dtype=np.int64))\n",
    "\n",
    "#Test\n",
    "n_test_samples = 5000\n",
    "test_sampler = torch.utils.data.sampler.SubsetRandomSampler(np.arange(n_test_samples, dtype=np.int64))\n",
    "\n",
    "\n",
    "def get_train_loader(batch):\n",
    "    return torch.utils.data.DataLoader(train_set, batch_size=batch,sampler=train_sampler, num_workers=2)\n",
    "\n",
    "cifar_test_loader = torch.utils.data.DataLoader(test_set, batch_size=4, sampler=test_sampler, num_workers=2)\n",
    "cifar_val_loader = torch.utils.data.DataLoader(train_set, batch_size=128, sampler=val_sampler, num_workers=2)\n",
    "cifar_train_loader = get_train_loader(batch_size)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once we have loaded the data sets in the correct form we can move onto training the model using our existing CNN\n",
    "class. We do this with both a new model and one that has had its first layer transferred from our earlier MNIST model\n",
    "so that we can compare results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.334187\n",
      "Train Epoch: 1 [12800/50000 (32%)]\tLoss: 2.317251\n",
      "Train Epoch: 1 [25600/50000 (64%)]\tLoss: 2.279302\n",
      "Train Epoch: 1 [38400/50000 (96%)]\tLoss: 2.334461\n",
      "\nTest set: Average loss: 0.2880, Accuracy: 503/10000 (5%)\n\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.306272\n",
      "Train Epoch: 1 [12800/50000 (32%)]\tLoss: 1.476375\n",
      "Train Epoch: 1 [25600/50000 (64%)]\tLoss: 1.221159\n",
      "Train Epoch: 1 [38400/50000 (96%)]\tLoss: 1.365536\n",
      "\nTest set: Average loss: 0.1589, Accuracy: 2817/10000 (28%)\n\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 2.288156\n",
      "Train Epoch: 2 [12800/50000 (32%)]\tLoss: 2.310925\n",
      "Train Epoch: 2 [25600/50000 (64%)]\tLoss: 2.323424\n",
      "Train Epoch: 2 [38400/50000 (96%)]\tLoss: 2.272743\n",
      "\nTest set: Average loss: 0.2880, Accuracy: 503/10000 (5%)\n\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.735711\n",
      "Train Epoch: 2 [12800/50000 (32%)]\tLoss: 1.574419\n",
      "Train Epoch: 2 [25600/50000 (64%)]\tLoss: 1.159616\n",
      "Train Epoch: 2 [38400/50000 (96%)]\tLoss: 1.271357\n",
      "\nTest set: Average loss: 0.1482, Accuracy: 3000/10000 (30%)\n\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 2.312783\n",
      "Train Epoch: 3 [12800/50000 (32%)]\tLoss: 2.304037\n",
      "Train Epoch: 3 [25600/50000 (64%)]\tLoss: 2.334486\n",
      "Train Epoch: 3 [38400/50000 (96%)]\tLoss: 2.312665\n",
      "\nTest set: Average loss: 0.2880, Accuracy: 503/10000 (5%)\n\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.513072\n",
      "Train Epoch: 3 [12800/50000 (32%)]\tLoss: 1.413137\n",
      "Train Epoch: 3 [25600/50000 (64%)]\tLoss: 1.122644\n",
      "Train Epoch: 3 [38400/50000 (96%)]\tLoss: 0.990430\n",
      "\nTest set: Average loss: 0.1412, Accuracy: 3063/10000 (31%)\n\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 2.298942\n",
      "Train Epoch: 4 [12800/50000 (32%)]\tLoss: 2.272328\n",
      "Train Epoch: 4 [25600/50000 (64%)]\tLoss: 2.288759\n",
      "Train Epoch: 4 [38400/50000 (96%)]\tLoss: 2.316433\n",
      "\nTest set: Average loss: 0.2880, Accuracy: 503/10000 (5%)\n\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.636463\n",
      "Train Epoch: 4 [12800/50000 (32%)]\tLoss: 1.096242\n",
      "Train Epoch: 4 [25600/50000 (64%)]\tLoss: 1.097488\n",
      "Train Epoch: 4 [38400/50000 (96%)]\tLoss: 1.060254\n",
      "\nTest set: Average loss: 0.1421, Accuracy: 3043/10000 (30%)\n\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 2.298499\n",
      "Train Epoch: 5 [12800/50000 (32%)]\tLoss: 2.313506\n",
      "Train Epoch: 5 [25600/50000 (64%)]\tLoss: 2.304817\n",
      "Train Epoch: 5 [38400/50000 (96%)]\tLoss: 2.304679\n",
      "\nTest set: Average loss: 0.2880, Accuracy: 503/10000 (5%)\n\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.948831\n",
      "Train Epoch: 5 [12800/50000 (32%)]\tLoss: 1.290766\n",
      "Train Epoch: 5 [25600/50000 (64%)]\tLoss: 1.495559\n",
      "Train Epoch: 5 [38400/50000 (96%)]\tLoss: 1.000622\n",
      "\nTest set: Average loss: 0.1309, Accuracy: 3233/10000 (32%)\n\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 2.330484\n",
      "Train Epoch: 6 [12800/50000 (32%)]\tLoss: 2.320712\n",
      "Train Epoch: 6 [25600/50000 (64%)]\tLoss: 2.314403\n",
      "Train Epoch: 6 [38400/50000 (96%)]\tLoss: 2.319985\n",
      "\nTest set: Average loss: 0.2880, Accuracy: 503/10000 (5%)\n\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 1.325981\n",
      "Train Epoch: 6 [12800/50000 (32%)]\tLoss: 1.196247\n",
      "Train Epoch: 6 [25600/50000 (64%)]\tLoss: 0.957951\n",
      "Train Epoch: 6 [38400/50000 (96%)]\tLoss: 1.073268\n",
      "\nTest set: Average loss: 0.1264, Accuracy: 3272/10000 (33%)\n\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 2.324596\n",
      "Train Epoch: 7 [12800/50000 (32%)]\tLoss: 2.282918\n",
      "Train Epoch: 7 [25600/50000 (64%)]\tLoss: 2.309481\n",
      "Train Epoch: 7 [38400/50000 (96%)]\tLoss: 2.319702\n",
      "\nTest set: Average loss: 0.2880, Accuracy: 503/10000 (5%)\n\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 1.277148\n",
      "Train Epoch: 7 [12800/50000 (32%)]\tLoss: 1.110190\n",
      "Train Epoch: 7 [25600/50000 (64%)]\tLoss: 1.163787\n",
      "Train Epoch: 7 [38400/50000 (96%)]\tLoss: 1.210326\n",
      "\nTest set: Average loss: 0.1323, Accuracy: 3182/10000 (32%)\n\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 2.306422\n",
      "Train Epoch: 8 [12800/50000 (32%)]\tLoss: 2.294740\n",
      "Train Epoch: 8 [25600/50000 (64%)]\tLoss: 2.299707\n",
      "Train Epoch: 8 [38400/50000 (96%)]\tLoss: 2.311821\n",
      "\nTest set: Average loss: 0.2880, Accuracy: 503/10000 (5%)\n\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 1.012955\n",
      "Train Epoch: 8 [12800/50000 (32%)]\tLoss: 1.069636\n",
      "Train Epoch: 8 [25600/50000 (64%)]\tLoss: 0.997605\n",
      "Train Epoch: 8 [38400/50000 (96%)]\tLoss: 0.892448\n",
      "\nTest set: Average loss: 0.1222, Accuracy: 3341/10000 (33%)\n\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 2.318245\n",
      "Train Epoch: 9 [12800/50000 (32%)]\tLoss: 2.292761\n",
      "Train Epoch: 9 [25600/50000 (64%)]\tLoss: 2.335516\n",
      "Train Epoch: 9 [38400/50000 (96%)]\tLoss: 2.310365\n",
      "\nTest set: Average loss: 0.2880, Accuracy: 503/10000 (5%)\n\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 1.467876\n",
      "Train Epoch: 9 [12800/50000 (32%)]\tLoss: 0.789508\n",
      "Train Epoch: 9 [25600/50000 (64%)]\tLoss: 0.803819\n",
      "Train Epoch: 9 [38400/50000 (96%)]\tLoss: 1.166155\n",
      "\nTest set: Average loss: 0.1243, Accuracy: 3286/10000 (33%)\n\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 2.277814\n",
      "Train Epoch: 10 [12800/50000 (32%)]\tLoss: 2.280558\n",
      "Train Epoch: 10 [25600/50000 (64%)]\tLoss: 2.285073\n",
      "Train Epoch: 10 [38400/50000 (96%)]\tLoss: 2.303686\n",
      "\nTest set: Average loss: 0.2880, Accuracy: 503/10000 (5%)\n\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 1.066049\n",
      "Train Epoch: 10 [12800/50000 (32%)]\tLoss: 0.666135\n",
      "Train Epoch: 10 [25600/50000 (64%)]\tLoss: 0.780346\n",
      "Train Epoch: 10 [38400/50000 (96%)]\tLoss: 0.938519\n",
      "\nTest set: Average loss: 0.1222, Accuracy: 3347/10000 (33%)\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "cifar_model = MNISTCNN().to(device)\n",
    "cifar_optimizer = torch.optim.Adam(cifar_model.parameters(), lr=learning_rate)\n",
    "cifar_loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "cifar_model_transfer = MNISTCNN().to(device)\n",
    "cifar_model_transfer.layer1 = mnist_model.layer1\n",
    "cifar_transfer_optimizer = torch.optim.Adam(cifar_model_transfer.parameters(), lr=learning_rate)\n",
    "cifar_transfer_loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "cifar_acc = []\n",
    "for epoch in range(1, cifar_epoch + 1):\n",
    "    #test_net(cifar_model,device,test_loader,loss_func)\n",
    "    train_net(cifar_model, device, cifar_train_loader, cifar_transfer_optimizer, loss_func, epoch)\n",
    "    cifar_acc.append(test_net(cifar_model, device, cifar_test_loader, loss_func))\n",
    "\n",
    "cifar_transfer_acc = []\n",
    "for epoch in range(1, cifar_epoch + 1):\n",
    "    train_net(cifar_model_transfer, device, cifar_train_loader, cifar_transfer_optimizer, loss_func, epoch)\n",
    "    cifar_transfer_acc.append(test_net(cifar_model_transfer, device, cifar_test_loader, loss_func))\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The graph below shows the difference in accuracy over the two CNNs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# GRAPH HERE    \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}