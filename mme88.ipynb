{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 401 Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "# imports\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms\n",
    "import torch.optim\n",
    "import torch.utils.data.sampler\n",
    "import torch.nn.functional\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# settings\n",
    "\n",
    "random.seed(55)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Gradient-based Learning with Tensors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining R<sup>5</sup> to R<sup>4</sup> Function\n",
    "The definition of the function with noise added is done in create_output_tensor_noisy. This takes a tensor input of\n",
    "with many different 5 value tuples and returns the output for all the tuples passed in.\n",
    "The R<sup>5</sup> to R<sup>4</sup> is defined by multiplying the input with a predefined weight matrix to apply\n",
    "a linear change.\n",
    " "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "def create_output_tensor_noisy(input, weights):\n",
    "    tensor_a = input#torch.from_numpy(input)\n",
    "    tensor_b = torch.from_numpy(weights)\n",
    "    output = tensor_a.t()@tensor_b.t()\n",
    "    noise = np.random.normal(loc=0, scale=0.01, size=np.shape(input.size))\n",
    "    output = output + torch.from_numpy(noise)\n",
    "    return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training\n",
    "We then train the network using tensor operations, our prediction / model is defined as transpose of input (x) matrix multiplied by\n",
    "the transpose of our current weight matrix then the addition of our biases.\n",
    "We calculate our loss using Mean Square Error (mse)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "epoch 50 loss = 130.42396545410156\nepoch 100 loss = 81.48611450195312\nepoch 150 loss = 51.027156829833984\nepoch 200 loss = 32.023921966552734\nepoch 250 loss = 20.140304565429688\nepoch 300 loss = 12.692172050476074\nepoch 350 loss = 8.013916969299316\nepoch 400 loss = 5.069363117218018\nepoch 450 loss = 3.21234393119812\nepoch 500 loss = 2.0389652252197266",
      "\nepoch 550 loss = 1.2962175607681274\nepoch 600 loss = 0.825257420539856\nepoch 650 loss = 0.5261430740356445\nepoch 700 loss = 0.33588260412216187\nepoch 750 loss = 0.2146846503019333\nepoch 800 loss = 0.1373753547668457\nepoch 850 loss = 0.08800213783979416\nepoch 900 loss = 0.056430935859680176\nepoch 950 loss = 0.036219652742147446\nepoch 1000 loss = 0.0232684426009655",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def mse(in1, in2):\n",
    "    diff = in1 - in2\n",
    "    return torch.sum(diff*diff) / diff.numel()\n",
    "\n",
    "def model(x,w,b):\n",
    "    return x.t() @ w.t() +b\n",
    "\n",
    "def train(input, target, w, b, i, learning_rate=1e-2):\n",
    "    prediction = model(input, w,b)\n",
    "    loss = mse(prediction, target)\n",
    "    if i % 50 == 0:\n",
    "        print(\"epoch \" + str(i) + \" loss = \" + str(loss.item()))\n",
    "    loss.backward(retain_graph=True)\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * learning_rate\n",
    "        b -= b.grad * learning_rate\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "def test(input, target, w, b):\n",
    "    prediction = model(input, w, b)\n",
    "    print('Expected:')\n",
    "    print(target.data.numpy())\n",
    "    print('Prediction:')\n",
    "    print(prediction.data.numpy())\n",
    "    return prediction\n",
    "\n",
    "\n",
    "\n",
    "eqtn = np.array([[3.7, 2.3, 1, 0, 5],\n",
    "      [4.7, 8.1, 2.5,  0, 25],\n",
    "      [2.7, 1, 9, 3.9, 0],\n",
    "      [5.2, 6, 2, 0, 0]], dtype=np.float32)\n",
    "tensor_in = torch.randn(5,1000, requires_grad=True)\n",
    "tensor_target = create_output_tensor_noisy(tensor_in, eqtn)     \n",
    "weights = torch.randn(4,5, requires_grad=True)\n",
    "biases = torch.randn(4, requires_grad=True)\n",
    "q1_epochs = 1000\n",
    "for i in range(1,q1_epochs+1):\n",
    "    train(input= tensor_in, target= tensor_target, w = weights, b = biases, i = i)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that after 1000 epochs over 1000 points of data the loss has been reduced to ~0.02\n",
    "\n",
    "Looking at a test we can see that the predicted values closely match what we expected, going further we can look at\n",
    "the final weights of the linear regression and compare them to the weights in the equation. Doing so we see that the \n",
    "found weights are very close to those defined by the equation showing that our network has found the correct linear\n",
    "regression formula."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Expected:\n[[  9.021386    32.55633     17.71089      9.463305  ]\n [ -2.3900907    6.9211836    0.41523686 -10.673137  ]\n [ -5.267981   -13.826017    -8.148701    -7.283438  ]\n [  3.3677537   27.09303     -0.9763158   -0.9655054 ]\n [ -2.3738487  -23.906734     5.9825783    1.1988251 ]]\nPrediction:\n[[  8.928804    32.20405     17.61855      9.363608  ]\n [ -2.405592     6.8294344    0.43143526 -10.636111  ]\n [ -5.22804    -13.791954    -8.110224    -7.2660575 ]\n [  3.2751114   26.712767    -0.91756266  -0.98951286]\n [ -2.362435   -23.6395       5.863574     1.1927851 ]]\n**********\nFinal MSE:\n0.0204462967813015\n**********\nKnown Linear Weights:\n[[ 3.7  2.3  1.   0.   5. ]\n [ 4.7  8.1  2.5  0.  25. ]\n [ 2.7  1.   9.   3.9  0. ]\n [ 5.2  6.   2.   0.   0. ]]\n****\nFound Linear Weights:\ntensor([[ 3.6772e+00,  2.2686e+00,  1.0056e+00, -1.1978e-02,  4.9500e+00],\n        [ 4.6702e+00,  7.9663e+00,  2.5148e+00,  3.6357e-02,  2.4741e+01],\n        [ 2.6657e+00,  1.0125e+00,  8.9412e+00,  3.8720e+00,  3.7527e-02],\n        [ 5.1638e+00,  5.9651e+00,  1.9852e+00,  9.2917e-03, -2.2444e-02]],\n       requires_grad=True)\n**********\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "tensor_test_in = torch.randn(5,5, requires_grad=True)\n",
    "tensor_test_target = create_output_tensor_noisy(tensor_test_in, eqtn)  \n",
    "q1_test_pred = test(tensor_test_in, tensor_test_target, weights, biases)\n",
    "print('**********')\n",
    "print('Final MSE:')\n",
    "print(mse(q1_test_pred, tensor_test_target).item())\n",
    "\n",
    "print('**********')\n",
    "print('Known Linear Weights:')\n",
    "print(eqtn)\n",
    "print('****')\n",
    "print('Found Linear Weights:')\n",
    "print(weights)\n",
    "print('**********')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Transfer Learning\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MNIST\n",
    "\n",
    "Transfer learning starts with the creation of a CNN for MNIST dataset, where we are able to train it to 99% correctness\n",
    "in one epoch of the 60,000 point dataset. This is a large increase from the 6% accuracy it has before training.\n",
    "\n",
    "Our CNN is defined in the class MNISTCNN which extends pytorch's Module class, providing the underlying logic for many\n",
    "CNN operations, allowing us to better define parameters such as inputs, layers and the forward function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "seed = 55\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "class MNISTCNN(torch.nn.Module): \n",
    "    # referenced site below for information regarding inputs and structure of class\n",
    "    # https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-in-pytorch/\n",
    "    def __init__(self):\n",
    "        super(MNISTCNN,self).__init__()\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1,32, kernel_size=5, stride=1, padding=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32,64, kernel_size=5, stride=1, padding=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.drop_out = torch.nn.Dropout()\n",
    "        self.fc1 = torch.nn.Linear(7 * 7 * 64, 1000)\n",
    "        self.fc2 = torch.nn.Linear(1000, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.drop_out(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " Next we train and test the CNN with the functions below. These functions iterate through the training and test loaders\n",
    " that are passed in, updating the model at each step. The training will then determine the loss between the prediction\n",
    " and target, using this and the loss function to update weights and biases accordingly at loss.backwards()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "def train_net(mnist_model, device, train_loader, optimizer, loss_func, epoch):\n",
    "    mnist_model.train()\n",
    "    train_loss = 0\n",
    "    count = 0\n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        count += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = mnist_model(data)\n",
    "        loss = loss_func(output, target)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i , len(train_loader),\n",
    "                100. * i / len(train_loader), loss.item()))\n",
    "    print('\\nTraining Set: Average loss: {:.4f}'.format(\n",
    "        train_loss/ count))\n",
    "    return train_loss/count\n",
    "\n",
    "def test_net(mnist_model, device, test_loader, loss_func):\n",
    "    mnist_model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            count += 1\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = mnist_model(data)\n",
    "            _, pred = torch.max(output.data,1)\n",
    "            test_loss += loss_func(output, target).item() # sum up batch loss\n",
    "            correct += pred.eq(target).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    #print(count)\n",
    "    return correct / len(test_loader.dataset)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Before Training\n",
      "\nTest set: Average loss: 0.0364, Accuracy: 623/10000 (6%)\n\nTraining model with:\nbatch size = 64\nlearning rate = 0.001\nover 1 epochs\nTrain Epoch: 1 [0/938 (0%)]\tLoss: 2.320465\n",
      "Train Epoch: 1 [100/938 (11%)]\tLoss: 0.091798\n",
      "Train Epoch: 1 [200/938 (21%)]\tLoss: 0.253857\n",
      "Train Epoch: 1 [300/938 (32%)]\tLoss: 0.132792\n",
      "Train Epoch: 1 [400/938 (43%)]\tLoss: 0.072163\n",
      "Train Epoch: 1 [500/938 (53%)]\tLoss: 0.094990\n",
      "Train Epoch: 1 [600/938 (64%)]\tLoss: 0.061900\n",
      "Train Epoch: 1 [700/938 (75%)]\tLoss: 0.091914\n",
      "Train Epoch: 1 [800/938 (85%)]\tLoss: 0.065473\n",
      "Train Epoch: 1 [900/938 (96%)]\tLoss: 0.160270\n",
      "\nTraining Set: Average loss: 0.1587\n",
      "\nTest set: Average loss: 0.0006, Accuracy: 9878/10000 (99%)\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_batch_size = 64\n",
    "test_batch_size = 100\n",
    "mnist_epochs = 1\n",
    "device = torch.device(\"cpu\")\n",
    "learning_rate = 1e-3\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "            torchvision.datasets.MNIST('./data', train=True, download=True,\n",
    "               transform=torchvision.transforms.transforms.Compose([\n",
    "                   torchvision.transforms.transforms.ToTensor(),\n",
    "                   torchvision.transforms.transforms.Normalize((0.1307,), (0.3081,))\n",
    "               ])),\n",
    "            batch_size=train_batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "            torchvision.datasets.MNIST('./data', train=False, transform=torchvision.transforms.transforms.Compose([\n",
    "                torchvision.transforms.transforms.ToTensor(),\n",
    "                torchvision.transforms.transforms.Normalize((0.1307,), (0.3081,))\n",
    "            ])),\n",
    "            batch_size=batch_size, shuffle=False)\n",
    "\n",
    "mnist_model = MNISTCNN().to(device)\n",
    "optimizer = torch.optim.Adam(mnist_model.parameters(), lr=learning_rate)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Before Training\")\n",
    "test_net(mnist_model,device,test_loader,loss_func)\n",
    "print(\"Training model with:\")\n",
    "print(\"batch size = \" + str(train_batch_size))\n",
    "print(\"learning rate = \" + str(learning_rate))\n",
    "print(\"over \" + str(mnist_epochs) + \" epochs\")\n",
    "for epoch in range(1, mnist_epochs + 1):\n",
    "    train_net(mnist_model, device, train_loader, optimizer, loss_func, epoch)\n",
    "    test_net(mnist_model, device, test_loader, loss_func)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CIFAR10\n",
    "To complete transfer learning we have to create a compatible CNN for our second data set, CIFAR10. This requires some\n",
    "form of transformation of the data as the data sets include images of different sizes and channels.\n",
    "This transformation can be seen in the transform variable below, where we use the Compose function to add several \n",
    "transforms including resizing the image and making it greyscale  so it has the same input parameters as the MNIST\n",
    "dataset.\n",
    "\n",
    "Changing the CIFAR10 dataset images to black and white to match the MNIST dataset was done as it is generally easier\n",
    "to convert a coloured image to greyscale then to do the reverse."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "seed = 55\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "class CIFARCNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFARCNN, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3,18, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = torch.nn.Linear(18*16*16, 64)\n",
    "        self.fc2 = torch.nn.Linear(64, 10)\n",
    "    def forward(self, x_val):\n",
    "        x_val = torch.nn.functional.relu(self.conv1(x_val))\n",
    "        x_val = self.pool(x_val)\n",
    "        x_val = x_val.view(-1, 18 * 16 * 16)\n",
    "        x_val = torch.nn.functional.relu(self.fc1(x_val))\n",
    "        x_val = self.fc2(x_val)\n",
    "        return x_val\n",
    "\n",
    "cifar_epoch = 10\n",
    "transform_cifar_to_mnist = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(size=28),\n",
    "    torchvision.transforms.Grayscale(num_output_channels=1),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_set_cifar_to_mnist = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cifar_to_mnist)\n",
    "test_set_cifar_to_mnist = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_cifar_to_mnist)\n",
    "\n",
    "transform_cifar = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "train_set_cifar= torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cifar)\n",
    "test_set_cifar = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_cifar)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# classes = ('plane', 'car', 'bird', 'cat',\n",
    "#           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "#Training\n",
    "n_training_samples = 50000\n",
    "train_sampler = torch.utils.data.sampler.SubsetRandomSampler(np.arange(n_training_samples, dtype=np.int64))\n",
    "#Test\n",
    "n_test_samples = 10000\n",
    "test_sampler = torch.utils.data.sampler.SubsetRandomSampler(np.arange(n_test_samples, dtype=np.int64))\n",
    "\n",
    "\n",
    "cifar_test_loader = torch.utils.data.DataLoader(test_set_cifar, batch_size=4, sampler=test_sampler, num_workers=2)\n",
    "cifar_train_loader = torch.utils.data.DataLoader(train_set_cifar, batch_size=64,sampler=train_sampler, num_workers=2)\n",
    "\n",
    "cifar_to_mnist_test_loader = torch.utils.data.DataLoader(test_set_cifar_to_mnist, batch_size=4, sampler=test_sampler, num_workers=2)\n",
    "cifar_to_mnist_train_loader = torch.utils.data.DataLoader(train_set_cifar_to_mnist, batch_size=64,sampler=train_sampler, num_workers=2)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once we have loaded the data sets in the correct form we can move onto training the model using our existing CNN\n",
    "class. We do this with both a new model and one that has had its first layer transferred from our earlier MNIST model\n",
    "so that we can compare results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Train Epoch: 1 [0/782 (0%)]\tLoss: 2.291676\n",
      "Train Epoch: 1 [100/782 (13%)]\tLoss: 1.669757\n",
      "Train Epoch: 1 [200/782 (26%)]\tLoss: 1.436748\n",
      "Train Epoch: 1 [300/782 (38%)]\tLoss: 1.390168\n",
      "Train Epoch: 1 [400/782 (51%)]\tLoss: 1.443785\n",
      "Train Epoch: 1 [500/782 (64%)]\tLoss: 1.383302\n",
      "Train Epoch: 1 [600/782 (77%)]\tLoss: 1.307558\n",
      "Train Epoch: 1 [700/782 (90%)]\tLoss: 1.322692\n",
      "\nTraining Set: Average loss: 1.4490\n",
      "\nTest set: Average loss: 0.3160, Accuracy: 5467/10000 (55%)\n\n",
      "Train Epoch: 2 [0/782 (0%)]\tLoss: 1.290075\n",
      "Train Epoch: 2 [100/782 (13%)]\tLoss: 1.323254\n",
      "Train Epoch: 2 [200/782 (26%)]\tLoss: 1.096234\n",
      "Train Epoch: 2 [300/782 (38%)]\tLoss: 1.365171\n",
      "Train Epoch: 2 [400/782 (51%)]\tLoss: 1.030478\n",
      "Train Epoch: 2 [500/782 (64%)]\tLoss: 1.208444\n",
      "Train Epoch: 2 [600/782 (77%)]\tLoss: 1.124928\n",
      "Train Epoch: 2 [700/782 (90%)]\tLoss: 1.006148\n",
      "\nTraining Set: Average loss: 1.1644\n",
      "\nTest set: Average loss: 0.2881, Accuracy: 5912/10000 (59%)\n\n",
      "Train Epoch: 3 [0/782 (0%)]\tLoss: 0.868991\n",
      "Train Epoch: 3 [100/782 (13%)]\tLoss: 1.093214\n",
      "Train Epoch: 3 [200/782 (26%)]\tLoss: 0.905331\n",
      "Train Epoch: 3 [300/782 (38%)]\tLoss: 1.312046\n",
      "Train Epoch: 3 [400/782 (51%)]\tLoss: 1.166313\n",
      "Train Epoch: 3 [500/782 (64%)]\tLoss: 1.206467\n",
      "Train Epoch: 3 [600/782 (77%)]\tLoss: 0.880983\n",
      "Train Epoch: 3 [700/782 (90%)]\tLoss: 0.826235\n",
      "\nTraining Set: Average loss: 1.0481\n",
      "\nTest set: Average loss: 0.2721, Accuracy: 6159/10000 (62%)\n\n",
      "Train Epoch: 4 [0/782 (0%)]\tLoss: 0.817490\n",
      "Train Epoch: 4 [100/782 (13%)]\tLoss: 0.936684\n",
      "Train Epoch: 4 [200/782 (26%)]\tLoss: 1.018401\n",
      "Train Epoch: 4 [300/782 (38%)]\tLoss: 1.332942\n",
      "Train Epoch: 4 [400/782 (51%)]\tLoss: 0.846493\n",
      "Train Epoch: 4 [500/782 (64%)]\tLoss: 1.023944\n",
      "Train Epoch: 4 [600/782 (77%)]\tLoss: 0.817326\n",
      "Train Epoch: 4 [700/782 (90%)]\tLoss: 0.999789\n",
      "\nTraining Set: Average loss: 0.9616\n",
      "\nTest set: Average loss: 0.2679, Accuracy: 6239/10000 (62%)\n\n",
      "Train Epoch: 5 [0/782 (0%)]\tLoss: 0.891411\n",
      "Train Epoch: 5 [100/782 (13%)]\tLoss: 0.885285\n",
      "Train Epoch: 5 [200/782 (26%)]\tLoss: 0.995667\n",
      "Train Epoch: 5 [300/782 (38%)]\tLoss: 0.952858\n",
      "Train Epoch: 5 [400/782 (51%)]\tLoss: 0.757909\n",
      "Train Epoch: 5 [500/782 (64%)]\tLoss: 0.932609\n",
      "Train Epoch: 5 [600/782 (77%)]\tLoss: 0.976767\n",
      "Train Epoch: 5 [700/782 (90%)]\tLoss: 1.084213\n",
      "\nTraining Set: Average loss: 0.8952\n",
      "\nTest set: Average loss: 0.2664, Accuracy: 6273/10000 (63%)\n\n",
      "Train Epoch: 6 [0/782 (0%)]\tLoss: 0.798436\n",
      "Train Epoch: 6 [100/782 (13%)]\tLoss: 0.789091\n",
      "Train Epoch: 6 [200/782 (26%)]\tLoss: 0.821665\n",
      "Train Epoch: 6 [300/782 (38%)]\tLoss: 0.651648\n",
      "Train Epoch: 6 [400/782 (51%)]\tLoss: 0.872220\n",
      "Train Epoch: 6 [500/782 (64%)]\tLoss: 0.974128\n",
      "Train Epoch: 6 [600/782 (77%)]\tLoss: 1.099656\n",
      "Train Epoch: 6 [700/782 (90%)]\tLoss: 1.051070\n",
      "\nTraining Set: Average loss: 0.8330\n",
      "\nTest set: Average loss: 0.2616, Accuracy: 6378/10000 (64%)\n\n",
      "Train Epoch: 7 [0/782 (0%)]\tLoss: 0.721944\n",
      "Train Epoch: 7 [100/782 (13%)]\tLoss: 0.661210\n",
      "Train Epoch: 7 [200/782 (26%)]\tLoss: 0.766361\n",
      "Train Epoch: 7 [300/782 (38%)]\tLoss: 0.842693\n",
      "Train Epoch: 7 [400/782 (51%)]\tLoss: 0.818627\n",
      "Train Epoch: 7 [500/782 (64%)]\tLoss: 0.685468\n",
      "Train Epoch: 7 [600/782 (77%)]\tLoss: 0.862656\n",
      "Train Epoch: 7 [700/782 (90%)]\tLoss: 0.759842\n",
      "\nTraining Set: Average loss: 0.7814\n",
      "\nTest set: Average loss: 0.2720, Accuracy: 6339/10000 (63%)\n\n",
      "Train Epoch: 8 [0/782 (0%)]\tLoss: 0.882482\n",
      "Train Epoch: 8 [100/782 (13%)]\tLoss: 0.671401\n",
      "Train Epoch: 8 [200/782 (26%)]\tLoss: 0.556015\n",
      "Train Epoch: 8 [300/782 (38%)]\tLoss: 0.775658\n",
      "Train Epoch: 8 [400/782 (51%)]\tLoss: 0.792723\n",
      "Train Epoch: 8 [500/782 (64%)]\tLoss: 0.970728\n",
      "Train Epoch: 8 [600/782 (77%)]\tLoss: 0.870971\n",
      "Train Epoch: 8 [700/782 (90%)]\tLoss: 0.738168\n",
      "\nTraining Set: Average loss: 0.7370\n",
      "\nTest set: Average loss: 0.2689, Accuracy: 6345/10000 (63%)\n\n",
      "Train Epoch: 9 [0/782 (0%)]\tLoss: 0.673248\n",
      "Train Epoch: 9 [100/782 (13%)]\tLoss: 0.735405\n",
      "Train Epoch: 9 [200/782 (26%)]\tLoss: 0.772947\n",
      "Train Epoch: 9 [300/782 (38%)]\tLoss: 0.514956\n",
      "Train Epoch: 9 [400/782 (51%)]\tLoss: 0.756114\n",
      "Train Epoch: 9 [500/782 (64%)]\tLoss: 0.766575\n",
      "Train Epoch: 9 [600/782 (77%)]\tLoss: 0.787236\n",
      "Train Epoch: 9 [700/782 (90%)]\tLoss: 0.673077\n",
      "\nTraining Set: Average loss: 0.6957\n",
      "\nTest set: Average loss: 0.2727, Accuracy: 6362/10000 (64%)\n\n",
      "Train Epoch: 10 [0/782 (0%)]\tLoss: 0.670163\n",
      "Train Epoch: 10 [100/782 (13%)]\tLoss: 0.724947\n",
      "Train Epoch: 10 [200/782 (26%)]\tLoss: 0.570701\n",
      "Train Epoch: 10 [300/782 (38%)]\tLoss: 0.490260\n",
      "Train Epoch: 10 [400/782 (51%)]\tLoss: 0.537922\n",
      "Train Epoch: 10 [500/782 (64%)]\tLoss: 0.837383\n",
      "Train Epoch: 10 [600/782 (77%)]\tLoss: 0.660795\n",
      "Train Epoch: 10 [700/782 (90%)]\tLoss: 0.528445\n",
      "\nTraining Set: Average loss: 0.6559\n",
      "\nTest set: Average loss: 0.2811, Accuracy: 6385/10000 (64%)\n\n",
      "Train Epoch: 1 [0/782 (0%)]\tLoss: 2.305367\n",
      "Train Epoch: 1 [100/782 (13%)]\tLoss: 1.700395\n",
      "Train Epoch: 1 [200/782 (26%)]\tLoss: 1.549722\n",
      "Train Epoch: 1 [300/782 (38%)]\tLoss: 1.588152\n",
      "Train Epoch: 1 [400/782 (51%)]\tLoss: 1.584133\n",
      "Train Epoch: 1 [500/782 (64%)]\tLoss: 1.307521\n",
      "Train Epoch: 1 [600/782 (77%)]\tLoss: 1.441359\n",
      "Train Epoch: 1 [700/782 (90%)]\tLoss: 1.226649\n",
      "\nTraining Set: Average loss: 1.5125\n",
      "\nTest set: Average loss: 0.3038, Accuracy: 5775/10000 (58%)\n\n",
      "Train Epoch: 2 [0/782 (0%)]\tLoss: 1.140761\n",
      "Train Epoch: 2 [100/782 (13%)]\tLoss: 1.542375\n",
      "Train Epoch: 2 [200/782 (26%)]\tLoss: 1.383356\n",
      "Train Epoch: 2 [300/782 (38%)]\tLoss: 1.115157\n",
      "Train Epoch: 2 [400/782 (51%)]\tLoss: 1.311908\n",
      "Train Epoch: 2 [500/782 (64%)]\tLoss: 1.142973\n",
      "Train Epoch: 2 [600/782 (77%)]\tLoss: 1.240174\n",
      "Train Epoch: 2 [700/782 (90%)]\tLoss: 1.341744\n",
      "\nTraining Set: Average loss: 1.2549\n",
      "\nTest set: Average loss: 0.2841, Accuracy: 6058/10000 (61%)\n\n",
      "Train Epoch: 3 [0/782 (0%)]\tLoss: 1.394656\n",
      "Train Epoch: 3 [100/782 (13%)]\tLoss: 1.518715\n",
      "Train Epoch: 3 [200/782 (26%)]\tLoss: 0.936512\n",
      "Train Epoch: 3 [300/782 (38%)]\tLoss: 1.214540\n",
      "Train Epoch: 3 [400/782 (51%)]\tLoss: 1.051427\n",
      "Train Epoch: 3 [500/782 (64%)]\tLoss: 1.250407\n",
      "Train Epoch: 3 [600/782 (77%)]\tLoss: 1.258258\n",
      "Train Epoch: 3 [700/782 (90%)]\tLoss: 1.268810\n",
      "\nTraining Set: Average loss: 1.1638\n",
      "\nTest set: Average loss: 0.2626, Accuracy: 6437/10000 (64%)\n\n",
      "Train Epoch: 4 [0/782 (0%)]\tLoss: 1.102745\n",
      "Train Epoch: 4 [100/782 (13%)]\tLoss: 0.933129\n",
      "Train Epoch: 4 [200/782 (26%)]\tLoss: 1.146246\n",
      "Train Epoch: 4 [300/782 (38%)]\tLoss: 1.233126\n",
      "Train Epoch: 4 [400/782 (51%)]\tLoss: 0.791801\n",
      "Train Epoch: 4 [500/782 (64%)]\tLoss: 0.954735\n",
      "Train Epoch: 4 [600/782 (77%)]\tLoss: 1.217022\n",
      "Train Epoch: 4 [700/782 (90%)]\tLoss: 1.197537\n",
      "\nTraining Set: Average loss: 1.1102\n",
      "\nTest set: Average loss: 0.2772, Accuracy: 6174/10000 (62%)\n\n",
      "Train Epoch: 5 [0/782 (0%)]\tLoss: 1.165374\n",
      "Train Epoch: 5 [100/782 (13%)]\tLoss: 1.105610\n",
      "Train Epoch: 5 [200/782 (26%)]\tLoss: 1.141287\n",
      "Train Epoch: 5 [300/782 (38%)]\tLoss: 0.933256\n",
      "Train Epoch: 5 [400/782 (51%)]\tLoss: 0.928870\n",
      "Train Epoch: 5 [500/782 (64%)]\tLoss: 1.209374\n",
      "Train Epoch: 5 [600/782 (77%)]\tLoss: 0.927489\n",
      "Train Epoch: 5 [700/782 (90%)]\tLoss: 0.957350\n",
      "\nTraining Set: Average loss: 1.0714\n",
      "\nTest set: Average loss: 0.2545, Accuracy: 6503/10000 (65%)\n\n",
      "Train Epoch: 6 [0/782 (0%)]\tLoss: 0.867384\n",
      "Train Epoch: 6 [100/782 (13%)]\tLoss: 1.472807\n",
      "Train Epoch: 6 [200/782 (26%)]\tLoss: 1.079821\n",
      "Train Epoch: 6 [300/782 (38%)]\tLoss: 1.067351\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "cifar_model = CIFARCNN().to(device)\n",
    "cifar_optimizer = torch.optim.Adam(cifar_model.parameters(), lr=learning_rate)\n",
    "cifar_loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "cifar_model_transfer = MNISTCNN().to(device)\n",
    "cifar_model_transfer.layer1 = mnist_model.layer1\n",
    "cifar_model_transfer.layer2 = mnist_model.layer2\n",
    "cifar_transfer_optimizer = torch.optim.Adam(cifar_model_transfer.parameters(), lr=learning_rate)\n",
    "cifar_transfer_loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "cifar_acc = []\n",
    "cifar_train_loss = []\n",
    "for epoch in range(1, cifar_epoch + 1):\n",
    "    cifar_train_loss.append(train_net(cifar_model, device, cifar_train_loader, cifar_optimizer, loss_func, epoch))\n",
    "    cifar_acc.append(test_net(cifar_model, device, cifar_test_loader, loss_func))\n",
    "\n",
    "cifar_transfer_acc = []\n",
    "cifar_transfer_train_loss = []\n",
    "for epoch in range(1, cifar_epoch + 1):\n",
    "    cifar_transfer_train_loss.append(train_net(cifar_model_transfer, device, cifar_to_mnist_train_loader, cifar_transfer_optimizer, cifar_transfer_loss_func, epoch))\n",
    "    cifar_transfer_acc.append(test_net(cifar_model_transfer, device, cifar_to_mnist_test_loader, cifar_transfer_loss_func))\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The graph below shows the accuracy each epoch of the two CNNs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# GRAPH HERE    \n",
    "print(\"Accuracy of networks over epochs\")\n",
    "plt.plot(cifar_acc,'r--', label='CIFAR10')\n",
    "plt.plot(cifar_transfer_acc, 'b-', label='Transfer Learning')\n",
    "plt.legend()\n",
    "plt.xticks(list(range(0,9)), list(range(1,10)))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The graph below shows the average training loss each epoch of the two CNNs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Average training loss over epochs\")\n",
    "plt.plot(cifar_train_loss,'r--', label='CIFAR10')\n",
    "plt.plot(cifar_transfer_train_loss, 'b-', label='Transfer Learning')\n",
    "plt.legend()\n",
    "plt.xticks(list(range(0,9)), list(range(1,10)))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Discussion\n",
    "In the graph above it is clearly shown that the transfered network (blue squares) performs better than the standard \n",
    "CIFAR10 CNN, this is especially noticeable in early iterations where the transferred layers from the MNIST set give a \n",
    "clear a advantage to the transfer learning CNN.\n",
    "\n",
    "The decrease in training loss over time shows the models are getting better at predicting the training set, we can also\n",
    "see that the CIFAR CNN trends much lower, this means that it is creating a 'better' model, however we can see from the\n",
    "accuracy above that the transfer learning out performs it, meaning that the CIFAR CNN is likely over-fitting the data\n",
    "more than the transfer learning cnn.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Cost-Sensitive Learning (Option A)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Pseudo code for loss function\n",
    "The loss can be defined as the absolute difference from the correct case where the prediction is right and the case of\n",
    "the prediction being wrong. Therefore the more incorrect a prediction the higher the loss value would be.\n",
    "\n",
    "When k >= 2 the loss function would still scale well as it is a simple index into the tensor/matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Python Pseudo Code (Not runnable)\n",
    "C = [[],[]] #cost matrix\n",
    "def q3_loss(pred, target):\n",
    "    if pred == target: \n",
    "        # the case of a correct classification\n",
    "        return 0\n",
    "    else:\n",
    "        math.fabs(C[target][target]- C[target][pred])\n",
    "         "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 Different way given learning algorithm\n",
    "Assuming we do not have control over the learning algorithm they only way for us to affect the optimisation is to\n",
    "manipulate the inputs. Considering we have a cost matrix C, we know which pairs of input and outputs are higher cost.\n",
    "To reduce the chance of these being used we can proportionally under-sample these data points, similarly with the inverse\n",
    "where low cost pairs in C can be over-sampled.\n",
    "\n",
    "One such algorithm for producing said data is SMOTE, which has been proven effective at low-dimensional data sets,\n",
    "described as k = 5 or 10. The same paper finds that it is not effective however for high dimensional data, though\n",
    "in this case where k = 100.\n",
    "\n",
    "SMOTE for High-Dimensional Class-Imbalanced Data\n",
    "Lara Lusa, March 2013\n",
    "https://www.researchgate.net/publication/236074132_SMOTE_for_High-Dimensional_Class-Imbalanced_Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}