{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 401 Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# imports\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms\n",
    "import torch.optim\n",
    "import torch.utils.data.sampler\n",
    "import torch.nn.functional\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "\n",
    "# settings\n",
    "\n",
    "random.seed(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Gradient-based Learning with Tensors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining R<sup>m</sup> to R<sup>n</sup> Function\n",
    " R<sup>m</sup> = 5 R<sup>n</sup> = 4 <br>\n",
    " all values m such that 1 <= m <= 10 <br>\n",
    " \n",
    " N1 = MA * (MB + MC) <br>\n",
    " N2 = MB * (MC + MD) <br>\n",
    " N3 = MC * (MD + ME) <br>\n",
    " N4 = MD * (ME + MA) <br>\n",
    " \n",
    " "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def m_to_n_list_noisy(m):\n",
    "    l = []\n",
    "    for a in m:\n",
    "        l.append(add_noise(m_to_n(a)))\n",
    "    return l\n",
    "    \n",
    "def m_to_n(m):\n",
    "    n = [\n",
    "        m[0]*(m[1]+m[2]), \n",
    "        m[1]*(m[2]+m[3]), \n",
    "        m[2]*(m[3]+m[4]), \n",
    "        m[3]*(m[4]+m[0])\n",
    "    ]\n",
    "    return n\n",
    "\n",
    "def add_noise(l):\n",
    "    for a in range(len(l)):\n",
    "        if random.random() > 0.5:\n",
    "            l[a] = l[a] ** ( 1 + random.random()/10)\n",
    "        else:\n",
    "            l[a] = l[a] ** ( 1 - random.random()/10)\n",
    "    return l\n",
    "            \n",
    "def generate_m_set(size_of_set, size_of_m):\n",
    "    r = []\n",
    "    for a in range(size_of_set):\n",
    "        r.append([])\n",
    "        for b in range(size_of_m):\n",
    "            r[a].append(random.randint(1, 10))\n",
    "    return r\n",
    "\n",
    "def get_m_n(size, m):\n",
    "    m = generate_m_set(size, m)\n",
    "    n = m_to_n_list_noisy(m)\n",
    "    return m, n\n",
    "\n",
    "# m = generate_m_set(100, 5)\n",
    "# print(m)\n",
    "# n = m_to_n_list_noisy(m)\n",
    "# print(n)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Numpy Attempt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-e539ba85eafb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (60,4) (60,5) "
     ],
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (60,4) (60,5) ",
     "output_type": "error"
    }
   ],
   "source": [
    "size = 100\n",
    "train_size = math.floor(size/5*3)\n",
    "x, y = get_m_n(size, 5)\n",
    "x_train, y_train = x[:train_size], y[:train_size]\n",
    "x_test, y_test = x[train_size:], y[train_size:]\n",
    "\n",
    "np.random.seed(42)\n",
    "a = np.random.randn(1)\n",
    "b = np.random.randn(1)\n",
    "learning_rate = 1e-1\n",
    "epochs = 100\n",
    "\n",
    "for i in range(epochs):\n",
    "    yhat = a + b * x_train\n",
    "    \n",
    "    error = (y_train - yhat)\n",
    "    \n",
    "    loss = (error ** 2).mean()\n",
    "    \n",
    "    a_grad = -2 * error.mean()\n",
    "    b_grad = -2 * (x_train * error).mean()\n",
    "    \n",
    "    a = a - learning_rate * a_grad\n",
    "    b = b - learning_rate * b_grad\n",
    "    \n",
    "print(a, b)\n",
    "    \n",
    "    \n",
    "# Sanity Check: do we get the same results as our gradient descent?\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linr = LinearRegression()\n",
    "linr.fit(x_train, y_train)\n",
    "print(linr.intercept_, linr.coef_[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Torch Attempt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot\n",
    "device = 'cpu'\n",
    "size = 100\n",
    "train_size = math.floor(size/5*3)\n",
    "\n",
    "x, y = get_m_n(size, 5)\n",
    "x_train, y_train = x[:train_size], y[:train_size]\n",
    "x_test, y_test = x[train_size:], y[train_size:]\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linr = LinearRegression()\n",
    "linr.fit(x_train, y_train)\n",
    "print(linr.intercept_, linr.coef_[0])\n",
    "\n",
    "x_train_tensor = torch.from_numpy(np.asarray(x_train)).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(np.asarray(y_train)).float().to(device)\n",
    "\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "epochs = 1000\n",
    "for e in range(epochs):\n",
    "    yhat = (a + b) * x_train_tensor\n",
    "    error = y_train_tensor - yhat\n",
    "    loss = (error ** 2).mean()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        \n",
    "    a.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    \n",
    "print(a,b)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Transfer Learning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CIFAR"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "===== HYPERPARAMETERS =====\nbatch_size= 32\nepochs= 5\nlearning_rate= 0.001\n==============================\n",
      "Epoch 1, 10% \t train_loss: 2.02 took: 3.70s\n",
      "Epoch 1, 20% \t train_loss: 1.79 took: 1.20s\n",
      "Epoch 1, 30% \t train_loss: 1.68 took: 1.21s\n",
      "Epoch 1, 40% \t train_loss: 1.64 took: 1.23s\n",
      "Epoch 1, 50% \t train_loss: 1.57 took: 1.21s\n",
      "Epoch 1, 60% \t train_loss: 1.51 took: 1.19s\n",
      "Epoch 1, 70% \t train_loss: 1.48 took: 1.21s\n",
      "Epoch 1, 80% \t train_loss: 1.46 took: 1.26s\n",
      "Epoch 1, 90% \t train_loss: 1.44 took: 1.21s\n",
      "Validation loss = 1.34\n",
      "Epoch 2, 10% \t train_loss: 1.29 took: 3.43s\n",
      "Epoch 2, 20% \t train_loss: 1.32 took: 1.21s\n",
      "Epoch 2, 30% \t train_loss: 1.32 took: 1.27s\n",
      "Epoch 2, 40% \t train_loss: 1.31 took: 1.19s\n",
      "Epoch 2, 50% \t train_loss: 1.30 took: 1.23s\n",
      "Epoch 2, 60% \t train_loss: 1.27 took: 1.20s\n",
      "Epoch 2, 70% \t train_loss: 1.26 took: 1.26s\n",
      "Epoch 2, 80% \t train_loss: 1.27 took: 1.20s\n",
      "Epoch 2, 90% \t train_loss: 1.27 took: 1.19s\n",
      "Validation loss = 1.27\n",
      "Epoch 3, 10% \t train_loss: 1.15 took: 3.58s\n",
      "Epoch 3, 20% \t train_loss: 1.15 took: 1.24s\n",
      "Epoch 3, 30% \t train_loss: 1.11 took: 1.26s\n",
      "Epoch 3, 40% \t train_loss: 1.16 took: 1.27s\n",
      "Epoch 3, 50% \t train_loss: 1.19 took: 1.27s\n",
      "Epoch 3, 60% \t train_loss: 1.17 took: 1.23s\n",
      "Epoch 3, 70% \t train_loss: 1.12 took: 1.22s\n",
      "Epoch 3, 80% \t train_loss: 1.13 took: 1.20s\n",
      "Epoch 3, 90% \t train_loss: 1.13 took: 1.22s\n",
      "Validation loss = 1.26\n",
      "Epoch 4, 10% \t train_loss: 1.01 took: 3.50s\n",
      "Epoch 4, 20% \t train_loss: 1.04 took: 1.21s\n",
      "Epoch 4, 30% \t train_loss: 1.04 took: 1.21s\n",
      "Epoch 4, 40% \t train_loss: 1.03 took: 1.22s\n",
      "Epoch 4, 50% \t train_loss: 1.05 took: 1.27s\n",
      "Epoch 4, 60% \t train_loss: 1.01 took: 1.28s\n",
      "Epoch 4, 70% \t train_loss: 1.04 took: 1.25s\n",
      "Epoch 4, 80% \t train_loss: 1.08 took: 1.29s\n",
      "Epoch 4, 90% \t train_loss: 1.04 took: 1.29s\n",
      "Validation loss = 1.20\n",
      "Epoch 5, 10% \t train_loss: 0.91 took: 3.53s\n",
      "Epoch 5, 20% \t train_loss: 0.88 took: 1.23s\n",
      "Epoch 5, 30% \t train_loss: 0.95 took: 1.21s\n",
      "Epoch 5, 40% \t train_loss: 0.94 took: 1.21s\n",
      "Epoch 5, 50% \t train_loss: 0.96 took: 1.22s\n",
      "Epoch 5, 60% \t train_loss: 0.97 took: 1.22s\n",
      "Epoch 5, 70% \t train_loss: 0.92 took: 1.25s\n",
      "Epoch 5, 80% \t train_loss: 0.97 took: 1.21s\n",
      "Epoch 5, 90% \t train_loss: 0.95 took: 1.22s\n",
      "Validation loss = 1.18\nTraining finished, took 88.54s\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "seed = 55\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "#The compose function allows for multiple transforms\n",
    "#transforms.ToTensor() converts our PILImage to a tensor of shape (C x H x W) in the range [0,1]\n",
    "#transforms.Normalize(mean,std) normalizes a tensor to a (mean, std) for (R, G, B)\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "train_set = torchvision.datasets.CIFAR10(root='./cifardata', train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.CIFAR10(root='./cifardata', train=False, download=True, transform=transform)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "#Training\n",
    "n_training_samples = 20000\n",
    "train_sampler = torch.utils.data.sampler.SubsetRandomSampler(np.arange(n_training_samples, dtype=np.int64))\n",
    "\n",
    "#Validation\n",
    "n_val_samples = 5000\n",
    "val_sampler = torch.utils.data.sampler.SubsetRandomSampler(np.arange(n_training_samples, n_training_samples + n_val_samples, dtype=np.int64))\n",
    "\n",
    "#Test\n",
    "n_test_samples = 5000\n",
    "test_sampler = torch.utils.data.sampler.SubsetRandomSampler(np.arange(n_test_samples, dtype=np.int64))\n",
    "\n",
    "\n",
    "def get_train_loader(batch):\n",
    "    return torch.utils.data.DataLoader(train_set, batch_size=batch,sampler=train_sampler, num_workers=2)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=4, sampler=test_sampler, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(train_set, batch_size=128, sampler=val_sampler, num_workers=2)\n",
    "\n",
    "\n",
    "#https://blog.algorithmia.com/convolutional-neural-nets-in-pytorch\n",
    "class MyCNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(3,18, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(18*16*16, 64)\n",
    "        \n",
    "        self.fc2 = torch.nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x_val):\n",
    "        \n",
    "        x_val = torch.nn.functional.relu(self.conv1(x_val))\n",
    "        \n",
    "        x_val = self.pool(x_val)\n",
    "        \n",
    "        x_val = x_val.view(-1, 18 * 16 * 16)\n",
    "        \n",
    "        x_val = torch.nn.functional.relu(self.fc1(x_val))\n",
    "\n",
    "        x_val = self.fc2(x_val)\n",
    "        \n",
    "        return x_val\n",
    "    \n",
    "def output_size(in_size, kernel_size, stride, padding):\n",
    "    output = int((in_size - kernel_size + 2*padding) / stride) + 1\n",
    "    return output\n",
    "\n",
    "def create_loss_optimiser(neural_net, learning_rate):\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    optimiser = torch.optim.Adam(neural_net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    return loss, optimiser\n",
    "\n",
    "def trainNet(net, batch_size, n_epochs, learning_rate):\n",
    "    \n",
    "    #Print all of the hyperparameters of the training iteration:\n",
    "    print(\"===== HYPERPARAMETERS =====\")\n",
    "    print(\"batch_size=\", batch_size)\n",
    "    print(\"epochs=\", n_epochs)\n",
    "    print(\"learning_rate=\", learning_rate)\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    #Get training data\n",
    "    train_loader = get_train_loader(batch_size)\n",
    "    n_batches = len(train_loader)\n",
    "    \n",
    "    #Create our loss and optimizer functions\n",
    "    loss, optimizer = create_loss_optimiser(net, learning_rate)\n",
    "    \n",
    "    #Time for printing\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    #Loop for n_epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        print_every = n_batches // 10\n",
    "        start_time = time.time()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            \n",
    "            #Get inputs\n",
    "            inputs, labels = data\n",
    "            \n",
    "            #Wrap them in a Variable object\n",
    "            inputs, labels = torch.autograd.Variable(inputs), torch.autograd.Variable(labels)\n",
    "            \n",
    "            #Set the parameter gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Forward pass, backward pass, optimize\n",
    "            outputs = net(inputs)\n",
    "            loss_size = loss(outputs, labels)\n",
    "            loss_size.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Print statistics\n",
    "            running_loss += loss_size.item()#.data[0]\n",
    "            total_train_loss += loss_size.item()#.data[0]\n",
    "\n",
    "            #Print every 10th batch of an epoch\n",
    "            if (i + 1) % (print_every + 1) == 0:\n",
    "                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
    "                        epoch+1, int(100 * (i+1) / n_batches), running_loss / print_every, time.time() - start_time))\n",
    "                #Reset running loss and time\n",
    "                running_loss = 0.0\n",
    "                start_time = time.time()\n",
    "            \n",
    "        #At the end of the epoch, do a pass on the validation set\n",
    "        total_val_loss = 0\n",
    "        for inputs, labels in val_loader:\n",
    "            \n",
    "            #Wrap tensors in Variables\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            #Forward pass\n",
    "            val_outputs = net(inputs)\n",
    "            val_loss_size = loss(val_outputs, labels)\n",
    "            total_val_loss += val_loss_size.item()#.data[0]\n",
    "            \n",
    "        print(\"Validation loss = {:.2f}\".format(total_val_loss / len(val_loader)))\n",
    "        \n",
    "    print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))\n",
    "\n",
    "\n",
    "CNN = MyCNN()\n",
    "trainNet(CNN, batch_size=32, n_epochs=5, learning_rate=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}