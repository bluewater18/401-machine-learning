{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 401 Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# imports\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms\n",
    "import torch.optim\n",
    "import torch.utils.data.sampler\n",
    "import torch.nn.functional\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "\n",
    "# settings\n",
    "\n",
    "random.seed(55)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Gradient-based Learning with Tensors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining R<sup>m</sup> to R<sup>n</sup> Function\n",
    " R<sup>m</sup> = 5 R<sup>n</sup> = 4 <br>\n",
    " "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "epoch 100 loss = tensor(88.3999, grad_fn=<DivBackward0>)\nepoch 200 loss = tensor(32.7373, grad_fn=<DivBackward0>)\nepoch 300 loss = tensor(12.3499, grad_fn=<DivBackward0>)\nepoch 400 loss = tensor(4.7476, grad_fn=<DivBackward0>)\nepoch 500 loss = tensor(1.8630, grad_fn=<DivBackward0>)\nepoch 600 loss = tensor(0.7483, grad_fn=<DivBackward0>)\n",
      "epoch 700 loss = tensor(0.3086, grad_fn=<DivBackward0>)\nepoch 800 loss = tensor(0.1311, grad_fn=<DivBackward0>)\nepoch 900 loss = tensor(0.0576, grad_fn=<DivBackward0>)\nepoch 1000 loss = tensor(0.0261, grad_fn=<DivBackward0>)\nepoch 1100 loss = tensor(0.0122, grad_fn=<DivBackward0>)\nepoch 1200 loss = tensor(0.0059, grad_fn=<DivBackward0>)\n",
      "epoch 1300 loss = tensor(0.0029, grad_fn=<DivBackward0>)\nepoch 1400 loss = tensor(0.0015, grad_fn=<DivBackward0>)\nepoch 1500 loss = tensor(0.0008, grad_fn=<DivBackward0>)\nepoch 1600 loss = tensor(0.0004, grad_fn=<DivBackward0>)\nepoch 1700 loss = tensor(0.0002, grad_fn=<DivBackward0>)\nepoch 1800 loss = tensor(0.0001, grad_fn=<DivBackward0>)\nepoch 1900 loss = tensor(6.0756e-05, grad_fn=<DivBackward0>)\n",
      "epoch 2000 loss = tensor(3.2692e-05, grad_fn=<DivBackward0>)\nExpected:\n[[ -1.9918872  -24.0012      -2.2709432    8.084901  ]\n [ -1.79479     -3.6399117    0.5344143   -0.60079765]\n [ -3.0514421   -6.715992     7.1263356   -1.2082739 ]\n [ -2.4938152    7.2537565   -9.794053    -5.4201226 ]\n [  4.139846    15.512672    14.28446     10.666717  ]\n [  8.263105    35.988243    13.041898    11.7537775 ]\n [ -4.22795    -38.064327     5.2488265   11.137204  ]\n [  2.9298303   16.542692    -2.965788     4.3399434 ]\n [ 19.060247    57.583416    20.46493     19.179468  ]\n [  2.2575948    2.9430296    4.662466    -2.7177153 ]]\nPrediction:\n[[ -1.9733334  -23.979792    -2.2593057    8.095333  ]\n [ -1.7758728   -3.6219509    0.55601513  -0.5785008 ]\n [ -3.0319993   -6.697783     7.1501584   -1.1830094 ]\n [ -2.4737148    7.2691507   -9.759114    -5.381788  ]\n [  4.15863     15.53094     14.303002    10.685837  ]\n [  8.282819    36.005127    13.069056    11.783567  ]\n [ -4.2081757  -38.041023     5.26295     11.151304  ]\n [  2.9484622   16.559364    -2.943288     4.36324   ]\n [ 19.077076    57.60194     20.47118     19.183594  ]\n [  2.2752602    2.9618611    4.675213    -2.7063649 ]]\n[[ 3.7  2.3  1.   0.   5. ]\n [ 4.7  8.1  2.5  0.  25. ]\n [ 2.7  1.   9.   3.9  0. ]\n [ 5.2  6.   2.   0.   0. ]]\ntensor([[ 3.6991e+00,  2.3002e+00,  9.9997e-01, -4.4082e-04,  4.9996e+00],\n        [ 4.7011e+00,  8.0999e+00,  2.5005e+00,  7.7758e-05,  2.4998e+01],\n        [ 2.6921e+00,  1.0008e+00,  8.9982e+00,  3.8970e+00,  1.9238e-03],\n        [ 5.1904e+00,  6.0012e+00,  1.9981e+00, -3.7028e-03,  2.1533e-03]],\n       requires_grad=True)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def create_output_tensor_noisy(input, weights):\n",
    "    tensor_a = input#torch.from_numpy(input)\n",
    "    tensor_b = torch.from_numpy(weights)\n",
    "    output = tensor_a.t()@tensor_b.t()\n",
    "    noise = np.random.normal(loc=0, scale=0.01, size=np.shape(input.size))\n",
    "    output = output + torch.from_numpy(noise)\n",
    "    return output\n",
    "\n",
    "def mse(in1, in2):\n",
    "    diff = in1 - in2\n",
    "    return torch.sum(diff*diff) / diff.numel()\n",
    "\n",
    "def model(x,w,b):\n",
    "    return x.t() @ w.t() +b\n",
    "\n",
    "def train(input, target, w, b, i, learning_rate=1e-2):\n",
    "    prediction = model(input, w,b)\n",
    "    loss = mse(prediction, target)\n",
    "    if i % 100 == 0:\n",
    "        print(\"epoch \" + str(i) + \" loss = \" + str(loss))\n",
    "    loss.backward(retain_graph=True)\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * learning_rate\n",
    "        b -= b.grad * learning_rate\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "def test(input, target, w, b):\n",
    "    prediction = model(input, w, b)\n",
    "    print('Expected:')\n",
    "    print(target.data.numpy())\n",
    "    print('Prediction:')\n",
    "    print(prediction.data.numpy())\n",
    "\n",
    "\n",
    "\n",
    "eqtn = np.array([[3.7, 2.3, 1, 0, 5],\n",
    "      [4.7, 8.1, 2.5,  0, 25],\n",
    "      [2.7, 1, 9, 3.9, 0],\n",
    "      [5.2, 6, 2, 0, 0]], dtype=np.float32)\n",
    "tensor_in = torch.randn(5,100, requires_grad=True)\n",
    "tensor_target = create_output_tensor_noisy(tensor_in, eqtn)     \n",
    "weights = torch.randn(4,5, requires_grad=True)\n",
    "biases = torch.randn(4, requires_grad=True)\n",
    "\n",
    "for i in range(1,2001):\n",
    "    train(input= tensor_in, target= tensor_target, w = weights, b = biases, i = i)\n",
    "\n",
    "tensor_test_in = torch.randn(5,10, requires_grad=True)\n",
    "tensor_test_target = create_output_tensor_noisy(tensor_test_in, eqtn)  \n",
    "test(tensor_test_in, tensor_test_target, weights, biases)\n",
    "\n",
    "print('**********')\n",
    "print('known linear weights:')\n",
    "print(eqtn)\n",
    "print('****')\n",
    "print('found linear weights:')\n",
    "print(weights)\n",
    "print('**********')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Transfer Learning\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MNIST"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "\nTest set: Average loss: 0.0364, Accuracy: 623/10000 (6%)\n\nTrain Epoch: 1 [0/60000 (0%)]\tLoss: 2.320465\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 0.852432\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.443613\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.631681\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.279837\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.058500\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.214556\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.466350\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.201038\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.121833\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.091798\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.152624\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.190699\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.091981\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.310389\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.146955\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.182830\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.283139\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.279277\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.119677\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.253857\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.092513\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.357751\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.108491\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.120628\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.200204\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.186080\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.030988\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.018547\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.264075\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.132792\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.085545\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.070923\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.146332\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.050490\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.375415\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.100799\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.174820\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.306250\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.096095\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.072163\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.065136\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.089330\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.172581\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.118796\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.101325\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.052385\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.040790\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.119328\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.075386\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.094990\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.079472\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.149210\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.093536\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.123965\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.093226\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.145834\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.038370\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.121186\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.029111\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.061900\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.059747\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.135881\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.049198\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.402810\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.173572\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.067971\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.013464\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.100462\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.106555\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.091914\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.251323\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.065981\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.146787\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.094630\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.113630\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.071726\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.052185\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.131643\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.191327\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.065473\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.148385\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.423344\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.150183\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.113406\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.050527\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.038541\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.125041\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.057813\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.094051\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.160270\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.034928\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.152757\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.030323\n",
      "\nTest set: Average loss: 0.0006, Accuracy: 9878/10000 (99%)\n\n",
      "\nTest set: Average loss: 0.0006, Accuracy: 9878/10000 (99%)\n\nTrain Epoch: 2 [0/60000 (0%)]\tLoss: 0.092812\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.103238\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.138488\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.016724\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.084499\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.031112\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.027202\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.052157\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.018026\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.038361\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.103730\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.043828\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.064517\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.078085\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.046981\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.098590\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.260285\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.058512\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.041821\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.058362\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.076404\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.067770\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.068208\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.066772\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.069575\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.190661\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.064705\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.103313\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.182704\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.089239\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.025313\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.174780\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.026978\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.044202\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.245148\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.103075\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.005803\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.038387\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.099964\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.013371\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.156282\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.078301\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.161390\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.040757\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.089867\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.020462\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.079487\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.061312\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.123624\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.058231\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.180014\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.013291\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.068169\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.072863\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.073448\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.108099\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.014211\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.028024\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.011856\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.025828\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.051657\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.104167\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.011001\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.195821\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.237526\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.024161\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.037301\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.045389\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.073509\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.072528\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.165805\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.024871\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.040933\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.264195\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.015996\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.040773\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.045562\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.188881\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.035643\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.140939\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.075757\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.237002\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.096649\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.065662\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.097803\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.240750\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.444353\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.049089\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.056273\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.013632\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.004000\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.005973\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.241454\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.029774\n",
      "\nTest set: Average loss: 0.0006, Accuracy: 9873/10000 (99%)\n\n",
      "\nTest set: Average loss: 0.0006, Accuracy: 9873/10000 (99%)\n\nTrain Epoch: 3 [0/60000 (0%)]\tLoss: 0.008151\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.094352\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.025857\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.076328\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.050829\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.139937\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.335527\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.055265\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.027111\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.108799\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.018082\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.192939\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.015309\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.019411\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.010034\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.113284\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.116284\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.009444\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.045485\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.077411\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.200479\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.073307\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.218217\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.093855\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.175632\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.095581\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.074250\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.094595\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.053244\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.112503\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.139255\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.017869\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.334688\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.045463\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.034595\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.107871\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.087517\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.102152\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.024637\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.144392\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.039326\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.023499\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.039639\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.089258\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.020694\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.037965\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.104821\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.021510\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.151511\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.163008\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.015366\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.018730\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.042940\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.016368\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.072901\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.035834\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.028367\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.066033\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.111253\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.012906\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.015972\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.069139\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.040752\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.020578\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.014328\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.069731\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.011659\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.005934\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.074425\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.019207\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.076972\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.017069\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.075484\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.017533\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.099485\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.020907\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.082712\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.041544\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.003910\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.099686\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.003495\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.101147\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.060327\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.108215\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.119384\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.016169\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.163726\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.019649\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.048048\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.060945\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.038535\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.049544\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.110318\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.080607\n",
      "\nTest set: Average loss: 0.0005, Accuracy: 9896/10000 (99%)\n\n",
      "\nTest set: Average loss: 0.0005, Accuracy: 9896/10000 (99%)\n\nTrain Epoch: 4 [0/60000 (0%)]\tLoss: 0.079516\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.073950\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.008308\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.077208\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.166847\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.040458\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.161851\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.148023\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.006122\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.013661\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.147214\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.006718\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.087259\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.209238\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.009252\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.028493\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.051732\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.073794\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.167494\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.084848\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.162990\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.131718\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.098501\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.138632\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.031906\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.052465\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.129903\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.034130\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.021666\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.048227\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.012015\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.057758\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.073774\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.013829\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.054749\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.099971\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.010449\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.034381\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.021442\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.009208\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.014560\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.002767\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.046591\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.120325\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.056442\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.059301\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.007292\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.082384\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.005576\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.023198\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.089268\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.024175\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.049431\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.110876\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.006763\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.032411\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.072543\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.097986\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.000986\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.048517\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.043053\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.033913\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.099130\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.034797\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.159595\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.033105\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.009254\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.081575\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.268595\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.004817\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.085103\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.052147\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.015176\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.016972\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.112904\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.069910\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.059330\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.148459\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.005002\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.009134\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.042906\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.469504\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.018857\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.005905\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.018580\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.065377\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.041983\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.038350\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.077960\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.048181\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.061793\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.028646\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.182924\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.108842\n",
      "\nTest set: Average loss: 0.0005, Accuracy: 9899/10000 (99%)\n\n",
      "\nTest set: Average loss: 0.0005, Accuracy: 9899/10000 (99%)\n\nTrain Epoch: 5 [0/60000 (0%)]\tLoss: 0.007757\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.025283\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.042565\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.015310\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.051699\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.027547\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.076858\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.028021\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.096066\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.081196\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.104846\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.004193\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.061601\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.002006\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.001846\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.030661\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.088627\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.079088\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.041328\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.084999\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.013929\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.372887\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.067094\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.048072\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.024251\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.041056\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.004321\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.041307\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.011619\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.172003\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.009287\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.007389\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.022695\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.014951\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.121952\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.072044\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.013571\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.019096\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.191517\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.009533\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.017787\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.095543\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.004931\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.059630\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.049853\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.226746\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.006737\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.026116\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.013339\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.266642\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.037510\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.017719\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.100684\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.056660\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.005263\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.058510\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.008900\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.003430\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.047099\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.001180\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.029962\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.041987\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.058975\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.032526\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.143386\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.045509\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.039099\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.068081\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.115110\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.057229\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.024389\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.073542\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.024523\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.006438\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.043773\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.004273\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.005345\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.154886\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.052841\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.007230\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.024875\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.032613\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.041005\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.041524\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.076825\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.020910\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.112315\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.031310\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.057480\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.023421\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.066636\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.011573\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.002945\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.145732\n",
      "\nTest set: Average loss: 0.0005, Accuracy: 9895/10000 (99%)\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "seed = 55\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "class MNISTCNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTCNN,self).__init__()\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1,32, kernel_size=5, stride=1, padding=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32,64, kernel_size=5, stride=1, padding=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.drop_out = torch.nn.Dropout()\n",
    "        self.fc1 = torch.nn.Linear(7 * 7 * 64, 1000)\n",
    "        self.fc2 = torch.nn.Linear(1000, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.drop_out(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train_net(mnist_model, device, train_loader, optimizer, loss_func, epoch):\n",
    "    mnist_model.train()\n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = mnist_model(data)\n",
    "        loss = loss_func(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(data), len(train_loader.dataset),\n",
    "                100. * i / len(train_loader), loss.item()))\n",
    "\n",
    "def test_net(mnist_model, device, test_loader, loss_func):\n",
    "    mnist_model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = mnist_model(data)\n",
    "            _, pred = torch.max(output.data,1)\n",
    "            test_loss += loss_func(output, target).item() # sum up batch loss\n",
    "            correct += pred.eq(target).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "batch_size = 64\n",
    "train_batch_size = 64\n",
    "test_batch_size = 100\n",
    "epochs = 5\n",
    "device = torch.device(\"cpu\")\n",
    "learning_rate = 1e-3\n",
    "momentum = 0.5\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "            torchvision.datasets.MNIST('../data', train=True, download=True,\n",
    "               transform=torchvision.transforms.transforms.Compose([\n",
    "                   torchvision.transforms.transforms.ToTensor(),\n",
    "                   torchvision.transforms.transforms.Normalize((0.1307,), (0.3081,))\n",
    "               ])),\n",
    "            batch_size=train_batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "            torchvision.datasets.MNIST('../data', train=False, transform=torchvision.transforms.transforms.Compose([\n",
    "                torchvision.transforms.transforms.ToTensor(),\n",
    "                torchvision.transforms.transforms.Normalize((0.1307,), (0.3081,))\n",
    "            ])),\n",
    "            batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "mnist_model = MNISTCNN().to(device)\n",
    "optimizer = torch.optim.Adam(mnist_model.parameters(), lr=learning_rate)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    test_net(mnist_model,device,test_loader,loss_func)\n",
    "    train_net(mnist_model, device, train_loader, optimizer, loss_func, epoch)\n",
    "    test_net(mnist_model, device, test_loader, loss_func)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CIFAR10\n",
    "\n",
    "running cifar as a black and white image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-bb469c5eab19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0mcifar_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMNISTCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[0mcifar_optimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcifar_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[0mcifar_loss_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\morgan\\pycharmprojects\\401_mme88\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    424\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\morgan\\pycharmprojects\\401_mme88\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\morgan\\pycharmprojects\\401_mme88\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\morgan\\pycharmprojects\\401_mme88\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    222\u001b[0m                 \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m                     \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\morgan\\pycharmprojects\\401_mme88\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\morgan\\pycharmprojects\\401_mme88\\venv\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    190\u001b[0m             raise RuntimeError(\n\u001b[0;32m    191\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m--> 192\u001b[1;33m         \u001b[0m_check_driver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[0m_cudart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\morgan\\pycharmprojects\\401_mme88\\venv\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[1;34m()\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_check_driver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_isDriverSufficient'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_isDriverSufficient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_getDriverVersion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ],
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error"
    }
   ],
   "source": [
    "seed = 55\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cpu\")\n",
    "#The compose function allows for multiple transforms\n",
    "#transforms.ToTensor() converts our PILImage to a tensor of shape (C x H x W) in the range [0,1]\n",
    "#transforms.Normalize(mean,std) normalizes a tensor to a (mean, std) for (R, G, B)\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(size=28),\n",
    "    torchvision.transforms.Grayscale(num_output_channels=1),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.1307,), (0.3081,))])#((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "train_set = torchvision.datasets.CIFAR10(root='./cifardata', train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.CIFAR10(root='./cifardata', train=False, download=True, transform=transform)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "#Training\n",
    "n_training_samples = 20000\n",
    "train_sampler = torch.utils.data.sampler.SubsetRandomSampler(np.arange(n_training_samples, dtype=np.int64))\n",
    "\n",
    "#Validation\n",
    "n_val_samples = 5000\n",
    "val_sampler = torch.utils.data.sampler.SubsetRandomSampler(np.arange(n_training_samples, n_training_samples + n_val_samples, dtype=np.int64))\n",
    "\n",
    "#Test\n",
    "n_test_samples = 5000\n",
    "test_sampler = torch.utils.data.sampler.SubsetRandomSampler(np.arange(n_test_samples, dtype=np.int64))\n",
    "\n",
    "\n",
    "def get_train_loader(batch):\n",
    "    return torch.utils.data.DataLoader(train_set, batch_size=batch,sampler=train_sampler, num_workers=2)\n",
    "\n",
    "cifar_test_loader = torch.utils.data.DataLoader(test_set, batch_size=4, sampler=test_sampler, num_workers=2)\n",
    "cifar_val_loader = torch.utils.data.DataLoader(train_set, batch_size=128, sampler=val_sampler, num_workers=2)\n",
    "cifar_train_loader = get_train_loader(batch_size)\n",
    "\n",
    "\n",
    "cifar_model = MNISTCNN().to(device)\n",
    "cifar_optimizer = torch.optim.Adam(cifar_model.parameters(), lr=learning_rate)\n",
    "cifar_loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "cifar_model_transfer = MNISTCNN().to(device)\n",
    "cifar_model_transfer.layer1 = mnist_model.layer1\n",
    "cifar_transfer_optimizer = torch.optim.Adam(cifar_model_transfer.parameters(), lr=learning_rate)\n",
    "cifar_transfer_loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    #test_net(cifar_model,device,test_loader,loss_func)\n",
    "    train_net(cifar_model, device, cifar_train_loader, cifar_transfer_optimizer, loss_func, epoch)\n",
    "    test_net(cifar_model, device, cifar_test_loader, loss_func)\n",
    "    train_net(cifar_model_transfer, device, cifar_train_loader, cifar_transfer_optimizer, loss_func, epoch)\n",
    "    test_net(cifar_model_transfer, device, cifar_test_loader, loss_func)\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}