{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 401 Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# imports\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms\n",
    "import torch.optim\n",
    "import torch.utils.data.sampler\n",
    "import torch.nn.functional\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "\n",
    "# settings\n",
    "\n",
    "random.seed(55)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Gradient-based Learning with Tensors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining R<sup>m</sup> to R<sup>n</sup> Function\n",
    " R<sup>m</sup> = 5 R<sup>n</sup> = 4 <br>\n",
    " "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "epoch 100 loss = tensor(88.3999, grad_fn=<DivBackward0>)\nepoch 200 loss = tensor(32.7373, grad_fn=<DivBackward0>)\nepoch 300 loss = tensor(12.3499, grad_fn=<DivBackward0>)\nepoch 400 loss = tensor(4.7476, grad_fn=<DivBackward0>)\nepoch 500 loss = tensor(1.8630, grad_fn=<DivBackward0>)\nepoch 600 loss = tensor(0.7483, grad_fn=<DivBackward0>)\n",
      "epoch 700 loss = tensor(0.3086, grad_fn=<DivBackward0>)\nepoch 800 loss = tensor(0.1311, grad_fn=<DivBackward0>)\nepoch 900 loss = tensor(0.0576, grad_fn=<DivBackward0>)\nepoch 1000 loss = tensor(0.0261, grad_fn=<DivBackward0>)\nepoch 1100 loss = tensor(0.0122, grad_fn=<DivBackward0>)\nepoch 1200 loss = tensor(0.0059, grad_fn=<DivBackward0>)\n",
      "epoch 1300 loss = tensor(0.0029, grad_fn=<DivBackward0>)\nepoch 1400 loss = tensor(0.0015, grad_fn=<DivBackward0>)\nepoch 1500 loss = tensor(0.0008, grad_fn=<DivBackward0>)\nepoch 1600 loss = tensor(0.0004, grad_fn=<DivBackward0>)\nepoch 1700 loss = tensor(0.0002, grad_fn=<DivBackward0>)\nepoch 1800 loss = tensor(0.0001, grad_fn=<DivBackward0>)\nepoch 1900 loss = tensor(6.0756e-05, grad_fn=<DivBackward0>)\n",
      "epoch 2000 loss = tensor(3.2692e-05, grad_fn=<DivBackward0>)\nExpected:\n[[ -1.9918872  -24.0012      -2.2709432    8.084901  ]\n [ -1.79479     -3.6399117    0.5344143   -0.60079765]\n [ -3.0514421   -6.715992     7.1263356   -1.2082739 ]\n [ -2.4938152    7.2537565   -9.794053    -5.4201226 ]\n [  4.139846    15.512672    14.28446     10.666717  ]\n [  8.263105    35.988243    13.041898    11.7537775 ]\n [ -4.22795    -38.064327     5.2488265   11.137204  ]\n [  2.9298303   16.542692    -2.965788     4.3399434 ]\n [ 19.060247    57.583416    20.46493     19.179468  ]\n [  2.2575948    2.9430296    4.662466    -2.7177153 ]]\nPrediction:\n[[ -1.9733334  -23.979792    -2.2593057    8.095333  ]\n [ -1.7758728   -3.6219509    0.55601513  -0.5785008 ]\n [ -3.0319993   -6.697783     7.1501584   -1.1830094 ]\n [ -2.4737148    7.2691507   -9.759114    -5.381788  ]\n [  4.15863     15.53094     14.303002    10.685837  ]\n [  8.282819    36.005127    13.069056    11.783567  ]\n [ -4.2081757  -38.041023     5.26295     11.151304  ]\n [  2.9484622   16.559364    -2.943288     4.36324   ]\n [ 19.077076    57.60194     20.47118     19.183594  ]\n [  2.2752602    2.9618611    4.675213    -2.7063649 ]]\n[[ 3.7  2.3  1.   0.   5. ]\n [ 4.7  8.1  2.5  0.  25. ]\n [ 2.7  1.   9.   3.9  0. ]\n [ 5.2  6.   2.   0.   0. ]]\ntensor([[ 3.6991e+00,  2.3002e+00,  9.9997e-01, -4.4082e-04,  4.9996e+00],\n        [ 4.7011e+00,  8.0999e+00,  2.5005e+00,  7.7758e-05,  2.4998e+01],\n        [ 2.6921e+00,  1.0008e+00,  8.9982e+00,  3.8970e+00,  1.9238e-03],\n        [ 5.1904e+00,  6.0012e+00,  1.9981e+00, -3.7028e-03,  2.1533e-03]],\n       requires_grad=True)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def create_output_tensor_noisy(input, weights):\n",
    "    tensor_a = input#torch.from_numpy(input)\n",
    "    tensor_b = torch.from_numpy(weights)\n",
    "    output = tensor_a.t()@tensor_b.t()\n",
    "    noise = np.random.normal(loc=0, scale=0.01, size=np.shape(input.size))\n",
    "    output = output + torch.from_numpy(noise)\n",
    "    return output\n",
    "\n",
    "def mse(in1, in2):\n",
    "    diff = in1 - in2\n",
    "    return torch.sum(diff*diff) / diff.numel()\n",
    "\n",
    "def model(x,w,b):\n",
    "    return x.t() @ w.t() +b\n",
    "\n",
    "def train(input, target, w, b, i, learning_rate=1e-2):\n",
    "    prediction = model(input, w,b)\n",
    "    loss = mse(prediction, target)\n",
    "    if i % 100 == 0:\n",
    "        print(\"epoch \" + str(i) + \" loss = \" + str(loss))\n",
    "    loss.backward(retain_graph=True)\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * learning_rate\n",
    "        b -= b.grad * learning_rate\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "def test(input, target, w, b):\n",
    "    prediction = model(input, w, b)\n",
    "    print('Expected:')\n",
    "    print(target.data.numpy())\n",
    "    print('Prediction:')\n",
    "    print(prediction.data.numpy())\n",
    "\n",
    "\n",
    "\n",
    "eqtn = np.array([[3.7, 2.3, 1, 0, 5],\n",
    "      [4.7, 8.1, 2.5,  0, 25],\n",
    "      [2.7, 1, 9, 3.9, 0],\n",
    "      [5.2, 6, 2, 0, 0]], dtype=np.float32)\n",
    "tensor_in = torch.randn(5,100, requires_grad=True)\n",
    "tensor_target = create_output_tensor_noisy(tensor_in, eqtn)     \n",
    "weights = torch.randn(4,5, requires_grad=True)\n",
    "biases = torch.randn(4, requires_grad=True)\n",
    "\n",
    "for i in range(1,2001):\n",
    "    train(input= tensor_in, target= tensor_target, w = weights, b = biases, i = i)\n",
    "\n",
    "tensor_test_in = torch.randn(5,10, requires_grad=True)\n",
    "tensor_test_target = create_output_tensor_noisy(tensor_test_in, eqtn)  \n",
    "test(tensor_test_in, tensor_test_target, weights, biases)\n",
    "\n",
    "print('**********')\n",
    "print('known linear weights:')\n",
    "print(eqtn)\n",
    "print('****')\n",
    "print('found linear weights:')\n",
    "print(weights)\n",
    "print('**********')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Transfer Learning\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MNIST"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-c69b623c9df2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m     \u001b[0mtrain_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m     \u001b[0mtest_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-54-c69b623c9df2>\u001b[0m in \u001b[0;36mtrain_net\u001b[1;34m(mnist_model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\morgan\\pycharmprojects\\401_mme88\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-54-c69b623c9df2>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x_val)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mx_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mx_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mx_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m4\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mx_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mx_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[-1, 800]' is invalid for input of size 1280"
     ],
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 800]' is invalid for input of size 1280",
     "output_type": "error"
    }
   ],
   "source": [
    "seed = 55\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "class MNISTNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTNet, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 20, 5, 1)\n",
    "        self.pool = torch.nn.MaxPool2d(20, 50, 5, 1)\n",
    "        self.fc1 = torch.nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = torch.nn.Linear(500, 10)\n",
    "    \n",
    "    def forward(self, x_val):\n",
    "        x_val = torch.nn.functional.relu(self.conv1(x_val))\n",
    "        x_val = torch.nn.functional.max_pool2d(x_val, 2, 2)\n",
    "        x_val = self.pool(x_val)\n",
    "        x_val = x_val.view(-1,4 * 4 * 50)\n",
    "        x_val = torch.nn.functional.relu(self.fc1(x_val))\n",
    "        x_val = self.fc2(x_val)\n",
    "        return x_val\n",
    "\n",
    "def train_net(mnist_model, device, train_loader, optimizer, epoch):\n",
    "    mnist_model.train()\n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = mnist_model(data)\n",
    "        loss = torch.nn.functional.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(data), len(train_loader.dataset),\n",
    "                100. * i / len(train_loader), loss.item()))\n",
    "\n",
    "def test_net(mnist_model, device, test_loader):\n",
    "    mnist_model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = mnist_model(data)\n",
    "            test_loss += torch.nn.functional.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "train_batch_size = 64\n",
    "test_batch_size = 100\n",
    "epochs = 10\n",
    "device = torch.device(\"cpu\")\n",
    "learning_rate = 1e-3\n",
    "momentum = 0.5\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "            torchvision.datasets.MNIST('../data', train=True, download=True,\n",
    "               transform=torchvision.transforms.transforms.Compose([\n",
    "                   torchvision.transforms.transforms.ToTensor(),\n",
    "                   torchvision.transforms.transforms.Normalize((0.1307,), (0.3081,))\n",
    "               ])),\n",
    "            batch_size=train_batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "            torchvision.datasets.MNIST('../data', train=False, transform=torchvision.transforms.transforms.Compose([\n",
    "                torchvision.transforms.transforms.ToTensor(),\n",
    "                torchvision.transforms.transforms.Normalize((0.1307,), (0.3081,))\n",
    "            ])),\n",
    "            batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "mnist_model = MNISTNet().to(device)\n",
    "optimizer = torch.optim.SGD(mnist_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_net(mnist_model, device, train_loader, optimizer, epoch)\n",
    "    test_net(mnist_model, device, test_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CIFAR"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed = 55\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "#The compose function allows for multiple transforms\n",
    "#transforms.ToTensor() converts our PILImage to a tensor of shape (C x H x W) in the range [0,1]\n",
    "#transforms.Normalize(mean,std) normalizes a tensor to a (mean, std) for (R, G, B)\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "train_set = torchvision.datasets.CIFAR10(root='./cifardata', train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.CIFAR10(root='./cifardata', train=False, download=True, transform=transform)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "#Training\n",
    "n_training_samples = 20000\n",
    "train_sampler = torch.utils.data.sampler.SubsetRandomSampler(np.arange(n_training_samples, dtype=np.int64))\n",
    "\n",
    "#Validation\n",
    "n_val_samples = 5000\n",
    "val_sampler = torch.utils.data.sampler.SubsetRandomSampler(np.arange(n_training_samples, n_training_samples + n_val_samples, dtype=np.int64))\n",
    "\n",
    "#Test\n",
    "n_test_samples = 5000\n",
    "test_sampler = torch.utils.data.sampler.SubsetRandomSampler(np.arange(n_test_samples, dtype=np.int64))\n",
    "\n",
    "\n",
    "def get_train_loader(batch):\n",
    "    return torch.utils.data.DataLoader(train_set, batch_size=batch,sampler=train_sampler, num_workers=2)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=4, sampler=test_sampler, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(train_set, batch_size=128, sampler=val_sampler, num_workers=2)\n",
    "\n",
    "\n",
    "#https://blog.algorithmia.com/convolutional-neural-nets-in-pytorch\n",
    "class MyCNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCNN, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3,18, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = torch.nn.Linear(18*16*16, 64)\n",
    "        self.fc2 = torch.nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x_val): \n",
    "        x_val = torch.nn.functional.relu(self.conv1(x_val))\n",
    "        x_val = self.pool(x_val)\n",
    "        x_val = x_val.view(-1, 18 * 16 * 16)\n",
    "        x_val = torch.nn.functional.relu(self.fc1(x_val))\n",
    "        x_val = self.fc2(x_val)\n",
    "        return x_val\n",
    "    \n",
    "def output_size(in_size, kernel_size, stride, padding):\n",
    "    output = int((in_size - kernel_size + 2*padding) / stride) + 1\n",
    "    return output\n",
    "\n",
    "def create_loss_optimiser(neural_net, learning_rate):\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    optimiser = torch.optim.Adam(neural_net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    return loss, optimiser\n",
    "\n",
    "def trainNet(net, batch_size, n_epochs, learning_rate):\n",
    "    \n",
    "    #Print all of the hyperparameters of the training iteration:\n",
    "    print(\"===== HYPERPARAMETERS =====\")\n",
    "    print(\"batch_size=\", batch_size)\n",
    "    print(\"epochs=\", n_epochs)\n",
    "    print(\"learning_rate=\", learning_rate)\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    #Get training data\n",
    "    train_loader = get_train_loader(batch_size)\n",
    "    n_batches = len(train_loader)\n",
    "    \n",
    "    #Create our loss and optimizer functions\n",
    "    loss, optimizer = create_loss_optimiser(net, learning_rate)\n",
    "    \n",
    "    #Time for printing\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    #Loop for n_epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        print_every = n_batches // 10\n",
    "        start_time = time.time()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            \n",
    "            #Get inputs\n",
    "            inputs, labels = data\n",
    "            \n",
    "            #Wrap them in a Variable object\n",
    "            inputs, labels = torch.autograd.Variable(inputs), torch.autograd.Variable(labels)\n",
    "            \n",
    "            #Set the parameter gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Forward pass, backward pass, optimize\n",
    "            outputs = net(inputs)\n",
    "            loss_size = loss(outputs, labels)\n",
    "            loss_size.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Print statistics\n",
    "            running_loss += loss_size.item()#.data[0]\n",
    "            total_train_loss += loss_size.item()#.data[0]\n",
    "\n",
    "            #Print every 10th batch of an epoch\n",
    "            if (i + 1) % (print_every + 1) == 0:\n",
    "                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
    "                        epoch+1, int(100 * (i+1) / n_batches), running_loss / print_every, time.time() - start_time))\n",
    "                #Reset running loss and time\n",
    "                running_loss = 0.0\n",
    "                start_time = time.time()\n",
    "            \n",
    "        #At the end of the epoch, do a pass on the validation set\n",
    "        total_val_loss = 0\n",
    "        for inputs, labels in val_loader:\n",
    "            \n",
    "            #Wrap tensors in Variables\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            #Forward pass\n",
    "            val_outputs = net(inputs)\n",
    "            val_loss_size = loss(val_outputs, labels)\n",
    "            total_val_loss += val_loss_size.item()#.data[0]\n",
    "            \n",
    "        print(\"Validation loss = {:.2f}\".format(total_val_loss / len(val_loader)))\n",
    "        \n",
    "    print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))\n",
    "\n",
    "\n",
    "CNN = MyCNN()\n",
    "trainNet(CNN, batch_size=32, n_epochs=5, learning_rate=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}